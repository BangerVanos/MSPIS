{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лабораторная работа 1 по дисциплине МРЗвИС\n",
    "# Выполнена студентом группы 121702\n",
    "# БГУИР Заломов Роман Андреевич\n",
    "#\n",
    "# Вариант 15: Реализовать модель линейной рециркуляционной сети \n",
    "# с постоянным коэффициентом обучения и нормированными весовыми коэффициентами.\n",
    "#\n",
    "# 21.10.2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RGB_VALUE = 255\n",
    "COLOR_CHANNELS_AMOUNT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_blocks(image, b_h, b_w, overlap = 0):\n",
    "    i_h, i_w = image.shape[:2]\n",
    "\n",
    "    step_h = int(b_h * (1 - overlap))\n",
    "    step_w = int(b_w * (1 - overlap))\n",
    "\n",
    "    blocks = []\n",
    "\n",
    "    for i in range(0, i_h - b_h + 1, step_h):\n",
    "        for j in range(0, i_w - b_w + 1, step_w):\n",
    "            block = image[i:i+b_h, j:j+b_w]                                  \n",
    "            blocks.append(block)    \n",
    "    \n",
    "    if i_h % b_h != 0:\n",
    "        for j in range(0, i_w - b_w + 1, step_w):\n",
    "            block = image[i_h-b_h:i_h, j:j+b_w]\n",
    "            blocks.append(block)    \n",
    "    \n",
    "    if i_w % b_w != 0:\n",
    "        for i in range(0, i_h - b_h + 1, step_h):\n",
    "            block = image[i:i+b_h, i_w-b_w:i_w]\n",
    "            blocks.append(block)    \n",
    "    \n",
    "    if i_h % b_h != 0 and i_w % b_w != 0:\n",
    "        block = image[i_h-b_h:i_h, i_w-b_w:i_w]\n",
    "        blocks.append(block)\n",
    "    \n",
    "    return np.asarray(blocks)\n",
    "\n",
    "\n",
    "def blocks_to_image(image_blocks, image_shape, b_h, b_w, overlap = 0):\n",
    "    i_h, i_w = image_shape[:2]\n",
    "    c = image_shape[2] if len(image_shape) == 3 else 1\n",
    "\n",
    "    restored_image = np.zeros((i_h, i_w, c), dtype=np.float64)\n",
    "    count_matrix = np.zeros((i_h, i_w), dtype=np.float64)\n",
    "    \n",
    "    step_h = int(b_h * (1 - overlap))\n",
    "    step_w = int(b_w * (1 - overlap))\n",
    "    \n",
    "    block_index = 0\n",
    "    \n",
    "    for i in range(0, i_h - b_h + 1, step_h):\n",
    "        for j in range(0, i_w - b_w + 1, step_w):\n",
    "            block = image_blocks[block_index]            \n",
    "            restored_image[i:i+b_h, j:j+b_w] += block\n",
    "            count_matrix[i:i+b_h, j:j+b_w] += 1\n",
    "            block_index += 1    \n",
    "    \n",
    "    if i_h % b_h != 0:\n",
    "        for j in range(0, i_w - b_w + 1, step_w):\n",
    "            block = image_blocks[block_index]\n",
    "            restored_image[i_h-b_h:i_h, j:j+b_w] += block\n",
    "            count_matrix[i_h-b_h:i_h, j:j+b_w] += 1\n",
    "            block_index += 1    \n",
    "    \n",
    "    if i_w % b_w != 0:\n",
    "        for i in range(0, i_h - b_h + 1, step_h):\n",
    "            block = image_blocks[block_index]\n",
    "            restored_image[i:i+b_h, i_w-b_w:i_w] += block\n",
    "            count_matrix[i:i+b_h, i_w-b_w:i_w] += 1\n",
    "            block_index += 1    \n",
    "    \n",
    "    if i_h % b_h != 0 and i_w % b_w != 0:\n",
    "        block = image_blocks[block_index]\n",
    "        restored_image[i_h-b_h:i_h, i_w-b_w:i_w] += block\n",
    "        count_matrix[i_h-b_h:i_h, i_w-b_w:i_w] += 1    \n",
    "    \n",
    "    count_matrix[count_matrix == 0] = 1    \n",
    "    restored_image = restored_image / count_matrix[..., np.newaxis]\n",
    "    restored_image[restored_image > 255] = 255    \n",
    "    \n",
    "    return restored_image.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weights(weights):\n",
    "    norms = np.linalg.norm(weights, axis=0)\n",
    "    return weights / norms\n",
    "\n",
    "# Функция активации\n",
    "def linear_activation(x):\n",
    "    return x\n",
    "\n",
    "class LRNN:\n",
    "    def __init__(self, input_dim, latent_dim, learning_rate=0.001):        \n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate        \n",
    "        \n",
    "        self.W_enc = normalize_weights(np.random.rand(self.input_dim, self.latent_dim))\n",
    "        self.W_dec = normalize_weights(np.random.rand(self.latent_dim, self.input_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = linear_activation(x @ self.W_enc)\n",
    "        x_reconstructed = linear_activation(z @ self.W_dec)\n",
    "        return z, x_reconstructed\n",
    "    \n",
    "    def backward(self, x, x_reconstructed):\n",
    "        error = x_reconstructed - x        \n",
    "        \n",
    "        dW_enc = (x.T @ error) @ self.W_dec.T\n",
    "        dW_dec = (x @ self.W_enc).T @ error               \n",
    "        \n",
    "        self.W_dec -= self.learning_rate * dW_dec\n",
    "        self.W_enc -= self.learning_rate * dW_enc        \n",
    "        \n",
    "        self.W_dec = normalize_weights(self.W_dec)\n",
    "        self.W_enc = normalize_weights(self.W_enc)\n",
    "    \n",
    "    def train(self, data, epochs=1000, max_loss: float = 100, learn_by_loss: bool = False):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x in data:                \n",
    "                x = np.matrix(x)\n",
    "                _, x_reconstructed = self.forward(x)\n",
    "                self.backward(x, x_reconstructed)\n",
    "                total_loss += np.sum(np.array(x - x_reconstructed) ** 2) # Put into loop\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss}')\n",
    "            if learn_by_loss and total_loss <= max_loss:\n",
    "                break            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image compression/decompression pipeline\n",
    "def compress_image(compression_weights, img_array, channels_amount: int,\n",
    "                   block_height: int, block_width: int, overlap: float = 0):    \n",
    "    normalized = (2.0 * img_array.astype(np.float32) / MAX_RGB_VALUE) - 1.0\n",
    "    blocks = image_to_blocks(normalized, block_height, block_width, overlap)\n",
    "    blocks = blocks.reshape((len(blocks), block_height * block_width, channels_amount))\n",
    "    if channels_amount == 3:\n",
    "        blocks = blocks.transpose(0, 2, 1)    \n",
    "    blocks = np.einsum('ijk,kl->ijl', blocks, compression_weights)     \n",
    "    return blocks\n",
    "    \n",
    "\n",
    "def decompress_image(decompression_weights, compressed_img, img_shape, channels_amount: int,\n",
    "                     block_height: int, block_width: int, overlap: float = 0) -> Image.Image:\n",
    "    compressed_img = np.einsum('ijk,kl->ijl', compressed_img, decompression_weights)\n",
    "    compressed_img = MAX_RGB_VALUE * (compressed_img + 1.0) / 2.0\n",
    "    if channels_amount == 3:\n",
    "        compressed_img = compressed_img.transpose(0, 2, 1)\n",
    "    compressed_img = compressed_img.reshape((len(compressed_img), block_height, block_width, channels_amount))    \n",
    "    img_array = blocks_to_image(compressed_img, img_shape, block_height, block_width, overlap)    \n",
    "    return Image.fromarray(img_array).convert('RGB' if channels_amount == 3 else 'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = 12.375931288829484\n",
      "Epoch 1/15000, Loss: 51345.69330123146\n",
      "Epoch 2/15000, Loss: 1697.8665155971976\n",
      "Epoch 3/15000, Loss: 1431.293984306756\n",
      "Epoch 4/15000, Loss: 1257.7569357654695\n",
      "Epoch 5/15000, Loss: 1130.8519019212783\n",
      "Epoch 6/15000, Loss: 1033.6705230592056\n",
      "Epoch 7/15000, Loss: 957.3549978451468\n",
      "Epoch 8/15000, Loss: 896.028855981155\n",
      "Epoch 9/15000, Loss: 845.5287192198356\n",
      "Epoch 10/15000, Loss: 802.9267697768316\n",
      "Epoch 11/15000, Loss: 766.1972621163878\n",
      "Epoch 12/15000, Loss: 733.9470020720629\n",
      "Epoch 13/15000, Loss: 705.2120720979303\n",
      "Epoch 14/15000, Loss: 679.31493316873\n",
      "Epoch 15/15000, Loss: 655.7681652295138\n",
      "Epoch 16/15000, Loss: 634.2110502013219\n",
      "Epoch 17/15000, Loss: 614.3682250294758\n",
      "Epoch 18/15000, Loss: 596.022831763301\n",
      "Epoch 19/15000, Loss: 578.9990832807754\n",
      "Epoch 20/15000, Loss: 563.1509032988887\n",
      "Epoch 21/15000, Loss: 548.3544627585756\n",
      "Epoch 22/15000, Loss: 534.5032007467731\n",
      "Epoch 23/15000, Loss: 521.5044207236414\n",
      "Epoch 24/15000, Loss: 509.27688222858774\n",
      "Epoch 25/15000, Loss: 497.74902327747174\n",
      "Epoch 26/15000, Loss: 486.85758768546174\n",
      "Epoch 27/15000, Loss: 476.5465199659607\n",
      "Epoch 28/15000, Loss: 466.7660454175631\n",
      "Epoch 29/15000, Loss: 457.4718862184127\n",
      "Epoch 30/15000, Loss: 448.6245837623634\n",
      "Epoch 31/15000, Loss: 440.1889084297568\n",
      "Epoch 32/15000, Loss: 432.1333439686299\n",
      "Epoch 33/15000, Loss: 424.4296368621488\n",
      "Epoch 34/15000, Loss: 417.0524027948942\n",
      "Epoch 35/15000, Loss: 409.9787833573485\n",
      "Epoch 36/15000, Loss: 403.1881468445728\n",
      "Epoch 37/15000, Loss: 396.6618276044121\n",
      "Epoch 38/15000, Loss: 390.38289895130555\n",
      "Epoch 39/15000, Loss: 384.33597520580486\n",
      "Epoch 40/15000, Loss: 378.5070389442615\n",
      "Epoch 41/15000, Loss: 372.8832900377571\n",
      "Epoch 42/15000, Loss: 367.453013514724\n",
      "Epoch 43/15000, Loss: 362.20546369152413\n",
      "Epoch 44/15000, Loss: 357.1307623773438\n",
      "Epoch 45/15000, Loss: 352.219809275011\n",
      "Epoch 46/15000, Loss: 347.4642029705383\n",
      "Epoch 47/15000, Loss: 342.8561711358342\n",
      "Epoch 48/15000, Loss: 338.388508765634\n",
      "Epoch 49/15000, Loss: 334.0545234361642\n",
      "Epoch 50/15000, Loss: 329.8479867137812\n",
      "Epoch 51/15000, Loss: 325.7630909608864\n",
      "Epoch 52/15000, Loss: 321.79441088725747\n",
      "Epoch 53/15000, Loss: 317.9368692806523\n",
      "Epoch 54/15000, Loss: 314.18570642351693\n",
      "Epoch 55/15000, Loss: 310.5364527651184\n",
      "Epoch 56/15000, Loss: 306.9849044720211\n",
      "Epoch 57/15000, Loss: 303.52710152605914\n",
      "Epoch 58/15000, Loss: 300.1593080789439\n",
      "Epoch 59/15000, Loss: 296.8779948073878\n",
      "Epoch 60/15000, Loss: 293.679823042888\n",
      "Epoch 61/15000, Loss: 290.56163047676233\n",
      "Epoch 62/15000, Loss: 287.52041826423243\n",
      "Epoch 63/15000, Loss: 284.5533393717281\n",
      "Epoch 64/15000, Loss: 281.6576880295222\n",
      "Epoch 65/15000, Loss: 278.83089016765905\n",
      "Epoch 66/15000, Loss: 276.07049472711014\n",
      "Epoch 67/15000, Loss: 273.3741657504828\n",
      "Epoch 68/15000, Loss: 270.7396751675747\n",
      "Epoch 69/15000, Loss: 268.16489620075214\n",
      "Epoch 70/15000, Loss: 265.6477973237811\n",
      "Epoch 71/15000, Loss: 263.18643671534073\n",
      "Epoch 72/15000, Loss: 260.77895715523215\n",
      "Epoch 73/15000, Loss: 258.4235813173026\n",
      "Epoch 74/15000, Loss: 256.1186074184132\n",
      "Epoch 75/15000, Loss: 253.86240518748573\n",
      "Epoch 76/15000, Loss: 251.6534121228443\n",
      "Epoch 77/15000, Loss: 249.4901300097324\n",
      "Epoch 78/15000, Loss: 247.37112167317332\n",
      "Epoch 79/15000, Loss: 245.29500794416793\n",
      "Epoch 80/15000, Loss: 243.2604648198063\n",
      "Epoch 81/15000, Loss: 241.26622080006277\n",
      "Epoch 82/15000, Loss: 239.31105438602708\n",
      "Epoch 83/15000, Loss: 237.39379172603174\n",
      "Epoch 84/15000, Loss: 235.51330439766332\n",
      "Epoch 85/15000, Loss: 233.6685073149574\n",
      "Epoch 86/15000, Loss: 231.8583567512466\n",
      "Epoch 87/15000, Loss: 230.08184846913989\n",
      "Epoch 88/15000, Loss: 228.3380159500134\n",
      "Epoch 89/15000, Loss: 226.6259287161462\n",
      "Epoch 90/15000, Loss: 224.94469073933976\n",
      "Epoch 91/15000, Loss: 223.2934389304288\n",
      "Epoch 92/15000, Loss: 221.6713417046302\n",
      "Epoch 93/15000, Loss: 220.07759761811562\n",
      "Epoch 94/15000, Loss: 218.51143407159213\n",
      "Epoch 95/15000, Loss: 216.97210607704102\n",
      "Epoch 96/15000, Loss: 215.45889508404377\n",
      "Epoch 97/15000, Loss: 213.97110786241433\n",
      "Epoch 98/15000, Loss: 212.50807543809714\n",
      "Epoch 99/15000, Loss: 211.06915207948907\n",
      "Epoch 100/15000, Loss: 209.65371433153757\n",
      "Epoch 101/15000, Loss: 208.26116009515547\n",
      "Epoch 102/15000, Loss: 206.890907749606\n",
      "Epoch 103/15000, Loss: 205.54239531569695\n",
      "Epoch 104/15000, Loss: 204.21507965770962\n",
      "Epoch 105/15000, Loss: 202.90843572212893\n",
      "Epoch 106/15000, Loss: 201.62195581132747\n",
      "Epoch 107/15000, Loss: 200.35514889047883\n",
      "Epoch 108/15000, Loss: 199.10753992603674\n",
      "Epoch 109/15000, Loss: 197.878669254223\n",
      "Epoch 110/15000, Loss: 196.6680919780507\n",
      "Epoch 111/15000, Loss: 195.47537739145685\n",
      "Epoch 112/15000, Loss: 194.30010842922036\n",
      "Epoch 113/15000, Loss: 193.1418811413912\n",
      "Epoch 114/15000, Loss: 192.0003041910277\n",
      "Epoch 115/15000, Loss: 190.8749983740873\n",
      "Epoch 116/15000, Loss: 189.76559616040097\n",
      "Epoch 117/15000, Loss: 188.67174125467346\n",
      "Epoch 118/15000, Loss: 187.5930881765551\n",
      "Epoch 119/15000, Loss: 186.52930185883574\n",
      "Epoch 120/15000, Loss: 185.48005726288727\n",
      "Epoch 121/15000, Loss: 184.44503901051303\n",
      "Epoch 122/15000, Loss: 183.42394103141802\n",
      "Epoch 123/15000, Loss: 182.41646622553287\n",
      "Epoch 124/15000, Loss: 181.42232613949523\n",
      "Epoch 125/15000, Loss: 180.44124065659764\n",
      "Epoch 126/15000, Loss: 179.4729376995721\n",
      "Epoch 127/15000, Loss: 178.51715294560253\n",
      "Epoch 128/15000, Loss: 177.5736295529832\n",
      "Epoch 129/15000, Loss: 176.64211789888546\n",
      "Epoch 130/15000, Loss: 175.72237532772564\n",
      "Epoch 131/15000, Loss: 174.81416590962218\n",
      "Epoch 132/15000, Loss: 173.91726020850652\n",
      "Epoch 133/15000, Loss: 173.031435059432\n",
      "Epoch 134/15000, Loss: 172.15647335467395\n",
      "Epoch 135/15000, Loss: 171.29216383822254\n",
      "Epoch 136/15000, Loss: 170.43830090830377\n",
      "Epoch 137/15000, Loss: 169.5946844275647\n",
      "Epoch 138/15000, Loss: 168.76111954059667\n",
      "Epoch 139/15000, Loss: 167.93741649847723\n",
      "Epoch 140/15000, Loss: 167.12339049002054\n",
      "Epoch 141/15000, Loss: 166.31886147946508\n",
      "Epoch 142/15000, Loss: 165.5236540503076\n",
      "Epoch 143/15000, Loss: 164.73759725504536\n",
      "Epoch 144/15000, Loss: 163.96052447055996\n",
      "Epoch 145/15000, Loss: 163.1922732589256\n",
      "Epoch 146/15000, Loss: 162.43268523340657\n",
      "Epoch 147/15000, Loss: 161.68160592943934\n",
      "Epoch 148/15000, Loss: 160.93888468039412\n",
      "Epoch 149/15000, Loss: 160.20437449791888\n",
      "Epoch 150/15000, Loss: 159.47793195669146\n",
      "Epoch 151/15000, Loss: 158.75941708339406\n",
      "Epoch 152/15000, Loss: 158.0486932497441\n",
      "Epoch 153/15000, Loss: 157.3456270694282\n",
      "Epoch 154/15000, Loss: 156.65008829877453\n",
      "Epoch 155/15000, Loss: 155.961949741024\n",
      "Epoch 156/15000, Loss: 155.28108715405884\n",
      "Epoch 157/15000, Loss: 154.60737916145067\n",
      "Epoch 158/15000, Loss: 153.94070716670282\n",
      "Epoch 159/15000, Loss: 153.2809552705573\n",
      "Epoch 160/15000, Loss: 152.62801019125703\n",
      "Epoch 161/15000, Loss: 151.98176118763894\n",
      "Epoch 162/15000, Loss: 151.34209998495814\n",
      "Epoch 163/15000, Loss: 150.70892070332908\n",
      "Epoch 164/15000, Loss: 150.08211978869036\n",
      "Epoch 165/15000, Loss: 149.46159594619564\n",
      "Epoch 166/15000, Loss: 148.8472500759318\n",
      "Epoch 167/15000, Loss: 148.23898521088216\n",
      "Epoch 168/15000, Loss: 147.63670645704482\n",
      "Epoch 169/15000, Loss: 147.04032093562327\n",
      "Epoch 170/15000, Loss: 146.4497377272164\n",
      "Epoch 171/15000, Loss: 145.86486781791947\n",
      "Epoch 172/15000, Loss: 145.2856240472765\n",
      "Epoch 173/15000, Loss: 144.711921058004\n",
      "Epoch 174/15000, Loss: 144.14367524742073\n",
      "Epoch 175/15000, Loss: 143.58080472052137\n",
      "Epoch 176/15000, Loss: 143.02322924462797\n",
      "Epoch 177/15000, Loss: 142.47087020555975\n",
      "Epoch 178/15000, Loss: 141.92365056526342\n",
      "Epoch 179/15000, Loss: 141.38149482085055\n",
      "Epoch 180/15000, Loss: 140.84432896498402\n",
      "Epoch 181/15000, Loss: 140.31208044756542\n",
      "Epoch 182/15000, Loss: 139.78467813867545\n",
      "Epoch 183/15000, Loss: 139.26205229271332\n",
      "Epoch 184/15000, Loss: 138.74413451369523\n",
      "Epoch 185/15000, Loss: 138.23085772166888\n",
      "Epoch 186/15000, Loss: 137.7221561201953\n",
      "Epoch 187/15000, Loss: 137.217965164864\n",
      "Epoch 188/15000, Loss: 136.71822153279973\n",
      "Epoch 189/15000, Loss: 136.2228630931213\n",
      "Epoch 190/15000, Loss: 135.73182887832348\n",
      "Epoch 191/15000, Loss: 135.24505905653893\n",
      "Epoch 192/15000, Loss: 134.76249490465125\n",
      "Epoch 193/15000, Loss: 134.2840787822299\n",
      "Epoch 194/15000, Loss: 133.8097541062518\n",
      "Epoch 195/15000, Loss: 133.33946532658206\n",
      "Epoch 196/15000, Loss: 132.8731579021865\n",
      "Epoch 197/15000, Loss: 132.41077827804747\n",
      "Epoch 198/15000, Loss: 131.95227386275909\n",
      "Epoch 199/15000, Loss: 131.49759300677533\n",
      "Epoch 200/15000, Loss: 131.04668498128729\n",
      "Epoch 201/15000, Loss: 130.59949995770822\n",
      "Epoch 202/15000, Loss: 130.15598898774073\n",
      "Epoch 203/15000, Loss: 129.71610398401208\n",
      "Epoch 204/15000, Loss: 129.27979770124892\n",
      "Epoch 205/15000, Loss: 128.84702371797707\n",
      "Epoch 206/15000, Loss: 128.41773641872865\n",
      "Epoch 207/15000, Loss: 127.99189097673512\n",
      "Epoch 208/15000, Loss: 127.56944333709133\n",
      "Epoch 209/15000, Loss: 127.15035020037439\n",
      "Epoch 210/15000, Loss: 126.73456900669953\n",
      "Epoch 211/15000, Loss: 126.32205792020238\n",
      "Epoch 212/15000, Loss: 125.91277581392703\n",
      "Epoch 213/15000, Loss: 125.50668225511086\n",
      "Epoch 214/15000, Loss: 125.10373749085005\n",
      "Epoch 215/15000, Loss: 124.70390243413604\n",
      "Epoch 216/15000, Loss: 124.30713865024732\n",
      "Epoch 217/15000, Loss: 123.91340834348475\n",
      "Epoch 218/15000, Loss: 123.52267434424427\n",
      "Epoch 219/15000, Loss: 123.13490009640934\n",
      "Epoch 220/15000, Loss: 122.75004964505716\n",
      "Epoch 221/15000, Loss: 122.3680876244668\n",
      "Epoch 222/15000, Loss: 121.9889792464215\n",
      "Epoch 223/15000, Loss: 121.6126902887918\n",
      "Epoch 224/15000, Loss: 121.23918708439747\n",
      "Epoch 225/15000, Loss: 120.86843651013284\n",
      "Epoch 226/15000, Loss: 120.50040597635027\n",
      "Epoch 227/15000, Loss: 120.13506341649598\n",
      "Epoch 228/15000, Loss: 119.77237727698714\n",
      "Epoch 229/15000, Loss: 119.41231650732502\n",
      "Epoch 230/15000, Loss: 119.05485055043589\n",
      "Epoch 231/15000, Loss: 118.69994933323493\n",
      "Epoch 232/15000, Loss: 118.34758325740395\n",
      "Epoch 233/15000, Loss: 117.99772319038073\n",
      "Epoch 234/15000, Loss: 117.65034045654893\n",
      "Epoch 235/15000, Loss: 117.30540682862598\n",
      "Epoch 236/15000, Loss: 116.9628945192443\n",
      "Epoch 237/15000, Loss: 116.62277617271799\n",
      "Epoch 238/15000, Loss: 116.28502485698867\n",
      "Epoch 239/15000, Loss: 115.94961405575323\n",
      "Epoch 240/15000, Loss: 115.61651766075553\n",
      "Epoch 241/15000, Loss: 115.28570996425341\n",
      "Epoch 242/15000, Loss: 114.95716565164001\n",
      "Epoch 243/15000, Loss: 114.63085979422878\n",
      "Epoch 244/15000, Loss: 114.30676784219125\n",
      "Epoch 245/15000, Loss: 113.98486561764214\n",
      "Epoch 246/15000, Loss: 113.66512930787394\n",
      "Epoch 247/15000, Loss: 113.34753545873171\n",
      "Epoch 248/15000, Loss: 113.03206096812765\n",
      "Epoch 249/15000, Loss: 112.71868307968926\n",
      "Epoch 250/15000, Loss: 112.4073793765411\n",
      "Epoch 251/15000, Loss: 112.098127775213\n",
      "Epoch 252/15000, Loss: 111.79090651967482\n",
      "Epoch 253/15000, Loss: 111.48569417549349\n",
      "Epoch 254/15000, Loss: 111.1824696241086\n",
      "Epoch 255/15000, Loss: 110.88121205722393\n",
      "Epoch 256/15000, Loss: 110.581900971313\n",
      "Epoch 257/15000, Loss: 110.28451616223545\n",
      "Epoch 258/15000, Loss: 109.98903771996058\n",
      "Epoch 259/15000, Loss: 109.69544602339907\n",
      "Epoch 260/15000, Loss: 109.40372173533241\n",
      "Epoch 261/15000, Loss: 109.11384579744872\n",
      "Epoch 262/15000, Loss: 108.82579942547443\n",
      "Epoch 263/15000, Loss: 108.53956410440075\n",
      "Epoch 264/15000, Loss: 108.25512158380532\n",
      "Epoch 265/15000, Loss: 107.97245387326615\n",
      "Epoch 266/15000, Loss: 107.69154323786312\n",
      "Epoch 267/15000, Loss: 107.41237219376988\n",
      "Epoch 268/15000, Loss: 107.13492350392741\n",
      "Epoch 269/15000, Loss: 106.8591801738052\n",
      "Epoch 270/15000, Loss: 106.58512544724178\n",
      "Epoch 271/15000, Loss: 106.31274280236548\n",
      "Epoch 272/15000, Loss: 106.04201594759496\n",
      "Epoch 273/15000, Loss: 105.77292881771403\n",
      "Epoch 274/15000, Loss: 105.50546557002289\n",
      "Epoch 275/15000, Loss: 105.23961058056175\n",
      "Epoch 276/15000, Loss: 104.97534844040669\n",
      "Epoch 277/15000, Loss: 104.71266395203575\n",
      "Epoch 278/15000, Loss: 104.45154212576163\n",
      "Epoch 279/15000, Loss: 104.19196817623401\n",
      "Epoch 280/15000, Loss: 103.933927519007\n",
      "Epoch 281/15000, Loss: 103.6774057671693\n",
      "Epoch 282/15000, Loss: 103.42238872803891\n",
      "Epoch 283/15000, Loss: 103.16886239991942\n",
      "Epoch 284/15000, Loss: 102.91681296891525\n",
      "Epoch 285/15000, Loss: 102.66622680580744\n",
      "Epoch 286/15000, Loss: 102.41709046298638\n",
      "Epoch 287/15000, Loss: 102.16939067144324\n",
      "Epoch 288/15000, Loss: 101.92311433781185\n",
      "Epoch 289/15000, Loss: 101.67824854147133\n",
      "Epoch 290/15000, Loss: 101.43478053169835\n",
      "Epoch 291/15000, Loss: 101.19269772487034\n",
      "Epoch 292/15000, Loss: 100.95198770172397\n",
      "Epoch 293/15000, Loss: 100.71263820466002\n",
      "Epoch 294/15000, Loss: 100.47463713509815\n",
      "Epoch 295/15000, Loss: 100.23797255088225\n",
      "Epoch 296/15000, Loss: 100.00263266372872\n",
      "Epoch 297/15000, Loss: 99.76860583672483\n"
     ]
    }
   ],
   "source": [
    "# Collecting everything\n",
    "\n",
    "block_width = 10\n",
    "block_height = 10\n",
    "\n",
    "n = block_height * block_width\n",
    "# Hidden layer neuron amount\n",
    "p = 64 \n",
    "\n",
    "img = Image.open('test_cat.jpg')\n",
    "img_array = np.asarray(img)\n",
    "shape = img_array.shape\n",
    "blocks = image_to_blocks(img_array, block_height, block_width, overlap=0)\n",
    "\n",
    "l = len(blocks)\n",
    "# Compression coeff\n",
    "print('Z =', (n*l) / ((n+l) * p+2))\n",
    "\n",
    "color_df = ((2 * blocks / MAX_RGB_VALUE) - 1).reshape(len(blocks), -1, 3).transpose(0, 2, 1).reshape(-1, 10 * 10)\n",
    "train = np.matrix(color_df[np.random.choice(color_df.shape[0], int(color_df.shape[0] * 0.05))])\n",
    "\n",
    "\n",
    "network = LRNN(100, 64, 0.001)\n",
    "network.train(train, 15000, learn_by_loss=True, max_loss=100)\n",
    "\n",
    "compressed = compress_image(network.W_enc, img_array, COLOR_CHANNELS_AMOUNT, 10, 10, 0)\n",
    "dimg = decompress_image(network.W_dec, compressed, shape, COLOR_CHANNELS_AMOUNT, 10, 10, 0)\n",
    "dimg_array = np.asarray(dimg)\n",
    "dimg.save('compression-decompression_test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on another pic\n",
    "img = Image.open('test5.jpg')\n",
    "img_array = np.asarray(img)\n",
    "shape = img_array.shape\n",
    "compressed = compress_image(network.W_enc, img_array, COLOR_CHANNELS_AMOUNT, 10, 10, 0)\n",
    "dimg = decompress_image(network.W_dec, compressed, shape, COLOR_CHANNELS_AMOUNT, 10, 10, 0)\n",
    "dimg_array = np.asarray(dimg)\n",
    "dimg.save('compression-decompression_test.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
