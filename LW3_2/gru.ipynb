{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate: float = 0.01,\n",
    "                 output_activation: Literal['linear', 'leaky_relu'] = 'linear',\n",
    "                 leaky_relu_alpha: float = 0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Инициализация весов для GRU\n",
    "        self.Wz = np.random.rand(hidden_size, input_size) * 0.01\n",
    "        self.Uz = np.random.rand(hidden_size, hidden_size) * 0.01\n",
    "        self.bz = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wh = np.random.rand(hidden_size, input_size) * 0.01\n",
    "        self.Uh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wr = np.random.rand(hidden_size, input_size) * 0.01\n",
    "        self.Ur = np.random.rand(hidden_size, hidden_size) * 0.01\n",
    "        self.br = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wo = np.random.rand(output_size, hidden_size) * 0.01\n",
    "        self.bo = np.zeros((output_size, 1))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Output neurons activation func + params\n",
    "        self.output_activation = output_activation\n",
    "        self.leaky_relu_alpha = leaky_relu_alpha\n",
    "\n",
    "    def activation(self, x):\n",
    "        # Гиперболический арксинус\n",
    "        return np.arcsinh(x)\n",
    "    \n",
    "    def leaky_relu(self, x, k: float = 0.01):\n",
    "        return np.where(x > 0, x, k * x)\n",
    "    \n",
    "    def linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.h = np.zeros((self.hidden_size, 1))\n",
    "        self.y = []\n",
    "        self.cache = []\n",
    "\n",
    "        for t in range(len(X)):\n",
    "            x_t = X[t].reshape(-1, 1)\n",
    "\n",
    "            # Грейдиентное обновление для GRU\n",
    "            z_t = self.sigmoid(self.Wz @ x_t + self.Uz @ self.h + self.bz)\n",
    "            r_t = self.sigmoid(self.Wr @ x_t + self.Ur @ self.h + self.br)\n",
    "            h_tilde = self.activation(self.Wh @ x_t + self.Uh @ (r_t * self.h) + self.bh)\n",
    "            h_next = (1 - z_t) * h_tilde + z_t * self.h\n",
    "            self.h = h_next\n",
    "\n",
    "            # Сохранение активаций для обратного распространения\n",
    "            self.cache.append((x_t, z_t, r_t, h_tilde, h_next))\n",
    "\n",
    "            # Выходное значение\n",
    "            y_t = self.Wo @ h_next + self.bo\n",
    "\n",
    "            if self.output_activation == 'linear':\n",
    "                y_t = self.linear(y_t)\n",
    "            elif self.output_activation == 'leaky_relu':\n",
    "                y_t = self.leaky_relu(y_t, self.leaky_relu_alpha)\n",
    "\n",
    "            self.y.append(y_t)\n",
    "\n",
    "        return np.array(self.y).squeeze(axis=-1)\n",
    "\n",
    "    def sigmoid(self, x):                \n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, X, Y, grad_clip_value=5.0):\n",
    "        dWz = np.zeros_like(self.Wz)\n",
    "        dUz = np.zeros_like(self.Uz)\n",
    "        dbz = np.zeros_like(self.bz)\n",
    "\n",
    "        dWh = np.zeros_like(self.Wh)\n",
    "        dUh = np.zeros_like(self.Uh)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "\n",
    "        dWr = np.zeros_like(self.Wr)\n",
    "        dUr = np.zeros_like(self.Ur)\n",
    "        dbr = np.zeros_like(self.br)\n",
    "\n",
    "        dWo = np.zeros_like(self.Wo)\n",
    "        dbo = np.zeros_like(self.bo)\n",
    "\n",
    "        dh_next = np.zeros_like(self.h)\n",
    "\n",
    "        # Обратное распространение ошибки по времени\n",
    "        for t in reversed(range(len(X))):\n",
    "            x_t, z_t, r_t, h_tilde, h_next = self.cache[t]\n",
    "            y_t = self.y[t]\n",
    "\n",
    "            # Ошибка на выходе\n",
    "            dy = y_t - Y[t]\n",
    "\n",
    "            # Градиенты для весов выхода\n",
    "            dWo += dy @ h_next.T\n",
    "            dbo += dy\n",
    "\n",
    "            # Градиенты для скрытого состояния\n",
    "            dh = self.Wo.T @ dy + dh_next\n",
    "            dh_tilde = dh * (1 - z_t)\n",
    "            dz = dh * (h_next - h_tilde)\n",
    "            dWh += dh_tilde * (1 - h_tilde ** 2) @ x_t.T\n",
    "            dUh += dh_tilde * (1 - h_tilde ** 2) @ (r_t * self.h).T\n",
    "            dbh += dh_tilde * (1 - h_tilde ** 2)\n",
    "\n",
    "            # Градиенты для обновлений\n",
    "            dUr += dz * r_t * self.h @ self.h.T\n",
    "            dWr += dz * r_t * self.h @ x_t.T\n",
    "            dbr += dz * r_t * self.h\n",
    "\n",
    "            # Градиенты для сброса и обновления\n",
    "            dUz += dz * (1 - z_t) @ self.h.T\n",
    "            dWz += dz * (1 - z_t) @ x_t.T\n",
    "            dbz += dz * (1 - z_t)\n",
    "\n",
    "            dh_next = (1 - z_t) * dh_tilde + z_t * dh\n",
    "        \n",
    "        # Ограничение градиентов, чтобы избежать взрыва\n",
    "        gradients = [dWz, dUz, dbz, dWh, dUh, dbh, dWr, dUr, dbr, dWo, dbo]\n",
    "        for grad in gradients:\n",
    "            np.clip(grad, -grad_clip_value, grad_clip_value, out=grad)\n",
    "\n",
    "        # Обновление весов\n",
    "        self.Wz -= self.learning_rate * dWz\n",
    "        self.Uz -= self.learning_rate * dUz\n",
    "        self.bz -= self.learning_rate * dbz\n",
    "\n",
    "        self.Wh -= self.learning_rate * dWh\n",
    "        self.Uh -= self.learning_rate * dUh\n",
    "        self.bh -= self.learning_rate * dbh\n",
    "\n",
    "        self.Wr -= self.learning_rate * dWr\n",
    "        self.Ur -= self.learning_rate * dUr\n",
    "        self.br -= self.learning_rate * dbr\n",
    "\n",
    "        self.Wo -= self.learning_rate * dWo\n",
    "        self.bo -= self.learning_rate * dbo\n",
    "\n",
    "    def train(self, X, Y, epochs=100, verbosity: int = 1, grad_clip_value: float = 5.0,\n",
    "              reset_hidden: bool = False):\n",
    "        for epoch in range(epochs):\n",
    "            # Reset hidden state each epoch if needed\n",
    "            if reset_hidden:\n",
    "                self.h = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "            self.forward(X)\n",
    "            self.backward(X, Y, grad_clip_value)\n",
    "            if epoch % verbosity == 0:\n",
    "                loss = np.mean((np.array(self.y) - Y) ** 2)\n",
    "                print(f'Epoch {epoch}/{epochs}, Loss: {loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data using sliding window\n",
    "def create_sliding_window_data(sequence, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence) - window_size):\n",
    "        X.append(sequence[i:i + window_size])\n",
    "        y.append(sequence[i + window_size])\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fibonacci sequence generator\n",
    "def fibonacci_generator(n):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        yield a\n",
    "        a, b = b, a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared num sequence generator\n",
    "def squared_generator(n, fst: float):\n",
    "    num = fst\n",
    "    for _ in range(n):\n",
    "        yield num\n",
    "        num = num**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared num sequence generator\n",
    "def half_generator(n, fst: float):\n",
    "    num = fst\n",
    "    for _ in range(n):\n",
    "        yield num\n",
    "        num /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/n sequence generator\n",
    "def one_by_n_generator(n):    \n",
    "    for i in range(n):\n",
    "        yield 1 / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, -1, 1, -1, 1,... sequence generator\n",
    "def plus_one_minus_one_generator(n):    \n",
    "    for i in range(n):        \n",
    "        yield 1 if i % 2 == 0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Fibonacci sequence and prepare data for prediction\n",
    "n = 10\n",
    "fibonacci_sequence = list(fibonacci_generator(n))\n",
    "fib_max = max(fibonacci_sequence)\n",
    "window_size = 3\n",
    "X_fib, y_fib = create_sliding_window_data(fibonacci_sequence, window_size)\n",
    "print(X_fib)\n",
    "print(y_fib)\n",
    "X_fib, y_fib = X_fib / fib_max, y_fib / fib_max\n",
    "print(X_fib)\n",
    "print(y_fib)\n",
    "\n",
    "\n",
    "# Predict the next number in the Fibonacci sequence\n",
    "fib_model = GRU(input_size=window_size, hidden_size=5, output_size=1, learning_rate=0.00001)\n",
    "fib_model.train(X_fib, y_fib, epochs=20000, verbosity=500, grad_clip_value=1)\n",
    "print(X_fib[-1].reshape(1, window_size))\n",
    "predicted_fib = fib_model.forward(X_fib[-1].reshape(1, window_size)).squeeze()\n",
    "print(predicted_fib)\n",
    "print(\"Predicted Fibonacci sequence:\", predicted_fib * fib_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/400000, Loss: 1.000002\n",
      "Epoch 1000/400000, Loss: 0.997968\n",
      "Epoch 2000/400000, Loss: 0.996268\n",
      "Epoch 3000/400000, Loss: 0.994849\n",
      "Epoch 4000/400000, Loss: 0.993663\n",
      "Epoch 5000/400000, Loss: 0.992673\n",
      "Epoch 6000/400000, Loss: 0.991846\n",
      "Epoch 7000/400000, Loss: 0.991155\n",
      "Epoch 8000/400000, Loss: 0.990579\n",
      "Epoch 9000/400000, Loss: 0.990097\n",
      "Epoch 10000/400000, Loss: 0.989694\n",
      "Epoch 11000/400000, Loss: 0.989358\n",
      "Epoch 12000/400000, Loss: 0.989077\n",
      "Epoch 13000/400000, Loss: 0.988843\n",
      "Epoch 14000/400000, Loss: 0.988647\n",
      "Epoch 15000/400000, Loss: 0.988483\n",
      "Epoch 16000/400000, Loss: 0.988347\n",
      "Epoch 17000/400000, Loss: 0.988233\n",
      "Epoch 18000/400000, Loss: 0.988137\n",
      "Epoch 19000/400000, Loss: 0.988058\n",
      "Epoch 20000/400000, Loss: 0.987991\n",
      "Epoch 21000/400000, Loss: 0.987936\n",
      "Epoch 22000/400000, Loss: 0.987889\n",
      "Epoch 23000/400000, Loss: 0.987850\n",
      "Epoch 24000/400000, Loss: 0.987818\n",
      "Epoch 25000/400000, Loss: 0.987791\n",
      "Epoch 26000/400000, Loss: 0.987769\n",
      "Epoch 27000/400000, Loss: 0.987750\n",
      "Epoch 28000/400000, Loss: 0.987734\n",
      "Epoch 29000/400000, Loss: 0.987721\n",
      "Epoch 30000/400000, Loss: 0.987710\n",
      "Epoch 31000/400000, Loss: 0.987701\n",
      "Epoch 32000/400000, Loss: 0.987693\n",
      "Epoch 33000/400000, Loss: 0.987687\n",
      "Epoch 34000/400000, Loss: 0.987681\n",
      "Epoch 35000/400000, Loss: 0.987677\n",
      "Epoch 36000/400000, Loss: 0.987673\n",
      "Epoch 37000/400000, Loss: 0.987670\n",
      "Epoch 38000/400000, Loss: 0.987667\n",
      "Epoch 39000/400000, Loss: 0.987665\n",
      "Epoch 40000/400000, Loss: 0.987664\n",
      "Epoch 41000/400000, Loss: 0.987662\n",
      "Epoch 42000/400000, Loss: 0.987661\n",
      "Epoch 43000/400000, Loss: 0.987660\n",
      "Epoch 44000/400000, Loss: 0.987659\n",
      "Epoch 45000/400000, Loss: 0.987658\n",
      "Epoch 46000/400000, Loss: 0.987658\n",
      "Epoch 47000/400000, Loss: 0.987658\n",
      "Epoch 48000/400000, Loss: 0.987657\n",
      "Epoch 49000/400000, Loss: 0.987657\n",
      "Epoch 50000/400000, Loss: 0.987657\n",
      "Epoch 51000/400000, Loss: 0.987657\n",
      "Epoch 52000/400000, Loss: 0.987657\n",
      "Epoch 53000/400000, Loss: 0.987658\n",
      "Epoch 54000/400000, Loss: 0.987658\n",
      "Epoch 55000/400000, Loss: 0.987659\n",
      "Epoch 56000/400000, Loss: 0.987660\n",
      "Epoch 57000/400000, Loss: 0.987661\n",
      "Epoch 58000/400000, Loss: 0.987662\n",
      "Epoch 59000/400000, Loss: 0.987663\n",
      "Epoch 60000/400000, Loss: 0.987665\n",
      "Epoch 61000/400000, Loss: 0.987668\n",
      "Epoch 62000/400000, Loss: 0.987671\n",
      "Epoch 63000/400000, Loss: 0.987674\n",
      "Epoch 64000/400000, Loss: 0.987678\n",
      "Epoch 65000/400000, Loss: 0.987683\n",
      "Epoch 66000/400000, Loss: 0.987689\n",
      "Epoch 67000/400000, Loss: 0.987697\n",
      "Epoch 68000/400000, Loss: 0.987706\n",
      "Epoch 69000/400000, Loss: 0.987716\n",
      "Epoch 70000/400000, Loss: 0.987729\n",
      "Epoch 71000/400000, Loss: 0.987745\n",
      "Epoch 72000/400000, Loss: 0.987764\n",
      "Epoch 73000/400000, Loss: 0.987787\n",
      "Epoch 74000/400000, Loss: 0.987815\n",
      "Epoch 75000/400000, Loss: 0.987848\n",
      "Epoch 76000/400000, Loss: 0.987888\n",
      "Epoch 77000/400000, Loss: 0.987937\n",
      "Epoch 78000/400000, Loss: 0.987995\n",
      "Epoch 79000/400000, Loss: 0.988064\n",
      "Epoch 80000/400000, Loss: 0.988148\n",
      "Epoch 81000/400000, Loss: 0.988248\n",
      "Epoch 82000/400000, Loss: 0.988368\n",
      "Epoch 83000/400000, Loss: 0.988510\n",
      "Epoch 84000/400000, Loss: 0.988681\n",
      "Epoch 85000/400000, Loss: 0.988884\n",
      "Epoch 86000/400000, Loss: 0.989125\n",
      "Epoch 87000/400000, Loss: 0.989411\n",
      "Epoch 88000/400000, Loss: 0.989747\n",
      "Epoch 89000/400000, Loss: 0.990140\n",
      "Epoch 90000/400000, Loss: 0.990597\n",
      "Epoch 91000/400000, Loss: 0.991126\n",
      "Epoch 92000/400000, Loss: 0.991736\n",
      "Epoch 93000/400000, Loss: 0.992439\n",
      "Epoch 94000/400000, Loss: 0.993245\n",
      "Epoch 95000/400000, Loss: 0.994166\n",
      "Epoch 96000/400000, Loss: 0.995217\n",
      "Epoch 97000/400000, Loss: 0.996412\n",
      "Epoch 98000/400000, Loss: 0.997765\n",
      "Epoch 99000/400000, Loss: 0.999296\n",
      "Epoch 100000/400000, Loss: 1.001020\n",
      "Epoch 101000/400000, Loss: 1.002958\n",
      "Epoch 102000/400000, Loss: 1.005130\n",
      "Epoch 103000/400000, Loss: 1.007557\n",
      "Epoch 104000/400000, Loss: 1.010261\n",
      "Epoch 105000/400000, Loss: 1.013263\n",
      "Epoch 106000/400000, Loss: 1.016579\n",
      "Epoch 107000/400000, Loss: 1.020229\n",
      "Epoch 108000/400000, Loss: 1.024235\n",
      "Epoch 109000/400000, Loss: 1.028619\n",
      "Epoch 110000/400000, Loss: 1.033404\n",
      "Epoch 111000/400000, Loss: 1.038611\n",
      "Epoch 112000/400000, Loss: 1.044260\n",
      "Epoch 113000/400000, Loss: 1.050373\n",
      "Epoch 114000/400000, Loss: 1.056969\n",
      "Epoch 115000/400000, Loss: 1.064066\n",
      "Epoch 116000/400000, Loss: 1.071681\n",
      "Epoch 117000/400000, Loss: 1.079830\n",
      "Epoch 118000/400000, Loss: 1.088527\n",
      "Epoch 119000/400000, Loss: 1.097784\n",
      "Epoch 120000/400000, Loss: 1.107610\n",
      "Epoch 121000/400000, Loss: 1.118012\n",
      "Epoch 122000/400000, Loss: 1.128996\n",
      "Epoch 123000/400000, Loss: 1.140563\n",
      "Epoch 124000/400000, Loss: 1.152713\n",
      "Epoch 125000/400000, Loss: 1.165442\n",
      "Epoch 126000/400000, Loss: 1.178745\n",
      "Epoch 127000/400000, Loss: 1.192611\n",
      "Epoch 128000/400000, Loss: 1.207029\n",
      "Epoch 129000/400000, Loss: 1.221984\n",
      "Epoch 130000/400000, Loss: 1.237459\n",
      "Epoch 131000/400000, Loss: 1.253433\n",
      "Epoch 132000/400000, Loss: 1.269884\n",
      "Epoch 133000/400000, Loss: 1.286787\n",
      "Epoch 134000/400000, Loss: 1.304114\n",
      "Epoch 135000/400000, Loss: 1.321785\n",
      "Epoch 136000/400000, Loss: 1.339733\n",
      "Epoch 137000/400000, Loss: 1.357917\n",
      "Epoch 138000/400000, Loss: 1.376293\n",
      "Epoch 139000/400000, Loss: 1.394818\n",
      "Epoch 140000/400000, Loss: 1.413451\n",
      "Epoch 141000/400000, Loss: 1.432147\n",
      "Epoch 142000/400000, Loss: 1.450791\n",
      "Epoch 143000/400000, Loss: 1.469226\n",
      "Epoch 144000/400000, Loss: 1.487409\n",
      "Epoch 145000/400000, Loss: 1.505301\n",
      "Epoch 146000/400000, Loss: 1.522866\n",
      "Epoch 147000/400000, Loss: 1.540074\n",
      "Epoch 148000/400000, Loss: 1.556896\n",
      "Epoch 149000/400000, Loss: 1.573310\n",
      "Epoch 150000/400000, Loss: 1.589294\n",
      "Epoch 151000/400000, Loss: 1.604832\n",
      "Epoch 152000/400000, Loss: 1.619910\n",
      "Epoch 153000/400000, Loss: 1.634519\n",
      "Epoch 154000/400000, Loss: 1.648650\n",
      "Epoch 155000/400000, Loss: 1.662299\n",
      "Epoch 156000/400000, Loss: 1.675465\n",
      "Epoch 157000/400000, Loss: 1.688146\n",
      "Epoch 158000/400000, Loss: 1.700345\n",
      "Epoch 159000/400000, Loss: 1.712066\n",
      "Epoch 160000/400000, Loss: 1.723316\n",
      "Epoch 161000/400000, Loss: 1.734100\n",
      "Epoch 162000/400000, Loss: 1.744428\n",
      "Epoch 163000/400000, Loss: 1.754309\n",
      "Epoch 164000/400000, Loss: 1.763753\n",
      "Epoch 165000/400000, Loss: 1.772772\n",
      "Epoch 166000/400000, Loss: 1.781378\n",
      "Epoch 167000/400000, Loss: 1.789582\n",
      "Epoch 168000/400000, Loss: 1.797398\n",
      "Epoch 169000/400000, Loss: 1.804839\n",
      "Epoch 170000/400000, Loss: 1.811917\n",
      "Epoch 171000/400000, Loss: 1.818646\n",
      "Epoch 172000/400000, Loss: 1.825039\n",
      "Epoch 173000/400000, Loss: 1.831110\n",
      "Epoch 174000/400000, Loss: 1.836871\n",
      "Epoch 175000/400000, Loss: 1.842336\n",
      "Epoch 176000/400000, Loss: 1.847516\n",
      "Epoch 177000/400000, Loss: 1.852425\n",
      "Epoch 178000/400000, Loss: 1.857074\n",
      "Epoch 179000/400000, Loss: 1.861476\n",
      "Epoch 180000/400000, Loss: 1.865642\n",
      "Epoch 181000/400000, Loss: 1.869583\n",
      "Epoch 182000/400000, Loss: 1.873309\n",
      "Epoch 183000/400000, Loss: 1.876832\n",
      "Epoch 184000/400000, Loss: 1.880161\n",
      "Epoch 185000/400000, Loss: 1.883306\n",
      "Epoch 186000/400000, Loss: 1.886276\n",
      "Epoch 187000/400000, Loss: 1.889081\n",
      "Epoch 188000/400000, Loss: 1.891728\n",
      "Epoch 189000/400000, Loss: 1.894226\n",
      "Epoch 190000/400000, Loss: 1.896583\n",
      "Epoch 191000/400000, Loss: 1.898806\n",
      "Epoch 192000/400000, Loss: 1.900903\n",
      "Epoch 193000/400000, Loss: 1.902880\n",
      "Epoch 194000/400000, Loss: 1.904743\n",
      "Epoch 195000/400000, Loss: 1.906500\n",
      "Epoch 196000/400000, Loss: 1.908155\n",
      "Epoch 197000/400000, Loss: 1.909715\n",
      "Epoch 198000/400000, Loss: 1.911184\n",
      "Epoch 199000/400000, Loss: 1.912569\n",
      "Epoch 200000/400000, Loss: 1.913872\n",
      "Epoch 201000/400000, Loss: 1.915100\n",
      "Epoch 202000/400000, Loss: 1.916256\n",
      "Epoch 203000/400000, Loss: 1.917345\n",
      "Epoch 204000/400000, Loss: 1.918370\n",
      "Epoch 205000/400000, Loss: 1.919336\n",
      "Epoch 206000/400000, Loss: 1.920244\n",
      "Epoch 207000/400000, Loss: 1.921100\n",
      "Epoch 208000/400000, Loss: 1.921905\n",
      "Epoch 209000/400000, Loss: 1.922663\n",
      "Epoch 210000/400000, Loss: 1.923376\n",
      "Epoch 211000/400000, Loss: 1.924048\n",
      "Epoch 212000/400000, Loss: 1.924680\n",
      "Epoch 213000/400000, Loss: 1.925276\n",
      "Epoch 214000/400000, Loss: 1.925836\n",
      "Epoch 215000/400000, Loss: 1.926364\n",
      "Epoch 216000/400000, Loss: 1.926861\n",
      "Epoch 217000/400000, Loss: 1.927328\n",
      "Epoch 218000/400000, Loss: 1.927769\n",
      "Epoch 219000/400000, Loss: 1.928184\n",
      "Epoch 220000/400000, Loss: 1.928575\n",
      "Epoch 221000/400000, Loss: 1.928943\n",
      "Epoch 222000/400000, Loss: 1.929291\n",
      "Epoch 223000/400000, Loss: 1.929618\n",
      "Epoch 224000/400000, Loss: 1.929926\n",
      "Epoch 225000/400000, Loss: 1.930217\n",
      "Epoch 226000/400000, Loss: 1.930492\n",
      "Epoch 227000/400000, Loss: 1.930751\n",
      "Epoch 228000/400000, Loss: 1.930996\n",
      "Epoch 229000/400000, Loss: 1.931227\n",
      "Epoch 230000/400000, Loss: 1.931445\n",
      "Epoch 231000/400000, Loss: 1.931651\n",
      "Epoch 232000/400000, Loss: 1.931846\n",
      "Epoch 233000/400000, Loss: 1.932031\n",
      "Epoch 234000/400000, Loss: 1.932205\n",
      "Epoch 235000/400000, Loss: 1.932371\n",
      "Epoch 236000/400000, Loss: 1.932528\n",
      "Epoch 237000/400000, Loss: 1.932676\n",
      "Epoch 238000/400000, Loss: 1.932818\n",
      "Epoch 239000/400000, Loss: 1.932952\n",
      "Epoch 240000/400000, Loss: 1.933079\n",
      "Epoch 241000/400000, Loss: 1.933201\n",
      "Epoch 242000/400000, Loss: 1.933316\n",
      "Epoch 243000/400000, Loss: 1.933426\n",
      "Epoch 244000/400000, Loss: 1.933532\n",
      "Epoch 245000/400000, Loss: 1.933632\n",
      "Epoch 246000/400000, Loss: 1.933728\n",
      "Epoch 247000/400000, Loss: 1.933820\n",
      "Epoch 248000/400000, Loss: 1.933908\n",
      "Epoch 249000/400000, Loss: 1.933993\n",
      "Epoch 250000/400000, Loss: 1.934074\n",
      "Epoch 251000/400000, Loss: 1.934152\n",
      "Epoch 252000/400000, Loss: 1.934227\n",
      "Epoch 253000/400000, Loss: 1.934300\n",
      "Epoch 254000/400000, Loss: 1.934370\n",
      "Epoch 255000/400000, Loss: 1.934438\n",
      "Epoch 256000/400000, Loss: 1.934503\n",
      "Epoch 257000/400000, Loss: 1.934567\n",
      "Epoch 258000/400000, Loss: 1.934629\n",
      "Epoch 259000/400000, Loss: 1.934689\n",
      "Epoch 260000/400000, Loss: 1.934747\n",
      "Epoch 261000/400000, Loss: 1.934804\n",
      "Epoch 262000/400000, Loss: 1.934860\n",
      "Epoch 263000/400000, Loss: 1.934915\n",
      "Epoch 264000/400000, Loss: 1.934968\n",
      "Epoch 265000/400000, Loss: 1.935020\n",
      "Epoch 266000/400000, Loss: 1.935072\n",
      "Epoch 267000/400000, Loss: 1.935122\n",
      "Epoch 268000/400000, Loss: 1.935172\n",
      "Epoch 269000/400000, Loss: 1.935221\n",
      "Epoch 270000/400000, Loss: 1.935269\n",
      "Epoch 271000/400000, Loss: 1.935317\n",
      "Epoch 272000/400000, Loss: 1.935364\n",
      "Epoch 273000/400000, Loss: 1.935410\n",
      "Epoch 274000/400000, Loss: 1.935456\n",
      "Epoch 275000/400000, Loss: 1.935502\n",
      "Epoch 276000/400000, Loss: 1.935548\n",
      "Epoch 277000/400000, Loss: 1.935593\n",
      "Epoch 278000/400000, Loss: 1.935637\n",
      "Epoch 279000/400000, Loss: 1.935682\n",
      "Epoch 280000/400000, Loss: 1.935726\n",
      "Epoch 281000/400000, Loss: 1.935771\n",
      "Epoch 282000/400000, Loss: 1.935815\n",
      "Epoch 283000/400000, Loss: 1.935858\n",
      "Epoch 284000/400000, Loss: 1.935902\n",
      "Epoch 285000/400000, Loss: 1.935946\n",
      "Epoch 286000/400000, Loss: 1.935990\n",
      "Epoch 287000/400000, Loss: 1.936033\n",
      "Epoch 288000/400000, Loss: 1.936077\n",
      "Epoch 289000/400000, Loss: 1.936121\n",
      "Epoch 290000/400000, Loss: 1.936164\n",
      "Epoch 291000/400000, Loss: 1.936208\n",
      "Epoch 292000/400000, Loss: 1.936252\n",
      "Epoch 293000/400000, Loss: 1.936295\n",
      "Epoch 294000/400000, Loss: 1.936339\n",
      "Epoch 295000/400000, Loss: 1.936383\n",
      "Epoch 296000/400000, Loss: 1.936427\n",
      "Epoch 297000/400000, Loss: 1.936471\n",
      "Epoch 298000/400000, Loss: 1.936515\n",
      "Epoch 299000/400000, Loss: 1.936560\n",
      "Epoch 300000/400000, Loss: 1.936604\n",
      "Epoch 301000/400000, Loss: 1.936648\n",
      "Epoch 302000/400000, Loss: 1.936693\n",
      "Epoch 303000/400000, Loss: 1.936738\n",
      "Epoch 304000/400000, Loss: 1.936783\n",
      "Epoch 305000/400000, Loss: 1.936828\n",
      "Epoch 306000/400000, Loss: 1.936873\n",
      "Epoch 307000/400000, Loss: 1.936918\n",
      "Epoch 308000/400000, Loss: 1.936964\n",
      "Epoch 309000/400000, Loss: 1.937009\n",
      "Epoch 310000/400000, Loss: 1.937055\n",
      "Epoch 311000/400000, Loss: 1.937101\n",
      "Epoch 312000/400000, Loss: 1.937147\n",
      "Epoch 313000/400000, Loss: 1.937193\n",
      "Epoch 314000/400000, Loss: 1.937240\n",
      "Epoch 315000/400000, Loss: 1.937286\n",
      "Epoch 316000/400000, Loss: 1.937333\n",
      "Epoch 317000/400000, Loss: 1.937380\n",
      "Epoch 318000/400000, Loss: 1.937427\n",
      "Epoch 319000/400000, Loss: 1.937474\n",
      "Epoch 320000/400000, Loss: 1.937521\n",
      "Epoch 321000/400000, Loss: 1.937569\n",
      "Epoch 322000/400000, Loss: 1.937617\n",
      "Epoch 323000/400000, Loss: 1.937664\n",
      "Epoch 324000/400000, Loss: 1.937712\n",
      "Epoch 325000/400000, Loss: 1.937760\n",
      "Epoch 326000/400000, Loss: 1.937809\n",
      "Epoch 327000/400000, Loss: 1.937857\n",
      "Epoch 328000/400000, Loss: 1.937905\n",
      "Epoch 329000/400000, Loss: 1.937954\n",
      "Epoch 330000/400000, Loss: 1.938003\n",
      "Epoch 331000/400000, Loss: 1.938052\n",
      "Epoch 332000/400000, Loss: 1.938101\n",
      "Epoch 333000/400000, Loss: 1.938150\n",
      "Epoch 334000/400000, Loss: 1.938200\n",
      "Epoch 335000/400000, Loss: 1.938249\n",
      "Epoch 336000/400000, Loss: 1.938299\n",
      "Epoch 337000/400000, Loss: 1.938349\n",
      "Epoch 338000/400000, Loss: 1.938398\n",
      "Epoch 339000/400000, Loss: 1.938449\n",
      "Epoch 340000/400000, Loss: 1.938499\n",
      "Epoch 341000/400000, Loss: 1.938549\n",
      "Epoch 342000/400000, Loss: 1.938599\n",
      "Epoch 343000/400000, Loss: 1.938650\n",
      "Epoch 344000/400000, Loss: 1.938701\n",
      "Epoch 345000/400000, Loss: 1.938751\n",
      "Epoch 346000/400000, Loss: 1.938802\n",
      "Epoch 347000/400000, Loss: 1.938853\n",
      "Epoch 348000/400000, Loss: 1.938904\n",
      "Epoch 349000/400000, Loss: 1.938956\n",
      "Epoch 350000/400000, Loss: 1.939007\n",
      "Epoch 351000/400000, Loss: 1.939058\n",
      "Epoch 352000/400000, Loss: 1.939110\n",
      "Epoch 353000/400000, Loss: 1.939162\n",
      "Epoch 354000/400000, Loss: 1.939213\n",
      "Epoch 355000/400000, Loss: 1.939265\n",
      "Epoch 356000/400000, Loss: 1.939317\n",
      "Epoch 357000/400000, Loss: 1.939369\n",
      "Epoch 358000/400000, Loss: 1.939421\n",
      "Epoch 359000/400000, Loss: 1.939474\n",
      "Epoch 360000/400000, Loss: 1.939526\n",
      "Epoch 361000/400000, Loss: 1.939578\n",
      "Epoch 362000/400000, Loss: 1.939631\n",
      "Epoch 363000/400000, Loss: 1.939683\n",
      "Epoch 364000/400000, Loss: 1.939736\n",
      "Epoch 365000/400000, Loss: 1.939789\n",
      "Epoch 366000/400000, Loss: 1.939841\n",
      "Epoch 367000/400000, Loss: 1.939894\n",
      "Epoch 368000/400000, Loss: 1.939947\n",
      "Epoch 369000/400000, Loss: 1.940000\n",
      "Epoch 370000/400000, Loss: 1.940053\n",
      "Epoch 371000/400000, Loss: 1.940107\n",
      "Epoch 372000/400000, Loss: 1.940160\n",
      "Epoch 373000/400000, Loss: 1.940213\n",
      "Epoch 374000/400000, Loss: 1.940267\n",
      "Epoch 375000/400000, Loss: 1.940320\n",
      "Epoch 376000/400000, Loss: 1.940373\n",
      "Epoch 377000/400000, Loss: 1.940427\n",
      "Epoch 378000/400000, Loss: 1.940481\n",
      "Epoch 379000/400000, Loss: 1.940534\n",
      "Epoch 380000/400000, Loss: 1.940588\n",
      "Epoch 381000/400000, Loss: 1.940642\n",
      "Epoch 382000/400000, Loss: 1.940696\n",
      "Epoch 383000/400000, Loss: 1.940750\n",
      "Epoch 384000/400000, Loss: 1.940803\n",
      "Epoch 385000/400000, Loss: 1.940857\n",
      "Epoch 386000/400000, Loss: 1.940911\n",
      "Epoch 387000/400000, Loss: 1.940966\n",
      "Epoch 388000/400000, Loss: 1.941020\n",
      "Epoch 389000/400000, Loss: 1.941074\n",
      "Epoch 390000/400000, Loss: 1.941128\n",
      "Epoch 391000/400000, Loss: 1.941182\n",
      "Epoch 392000/400000, Loss: 1.941236\n",
      "Epoch 393000/400000, Loss: 1.941291\n",
      "Epoch 394000/400000, Loss: 1.941345\n",
      "Epoch 395000/400000, Loss: 1.941399\n",
      "Epoch 396000/400000, Loss: 1.941454\n",
      "Epoch 397000/400000, Loss: 1.941508\n",
      "Epoch 398000/400000, Loss: 1.941563\n",
      "Epoch 399000/400000, Loss: 1.941617\n",
      "Test dataset validation\n",
      "MAE on test: 0.37270330\n"
     ]
    }
   ],
   "source": [
    "# Generate squared sequence\n",
    "output_activation = 'linear'\n",
    "output_activation_alpha = 0.01\n",
    "\n",
    "n = 15\n",
    "sequence = list(plus_one_minus_one_generator(n))\n",
    "window_size = 3\n",
    "X, y = create_sliding_window_data(sequence, window_size)\n",
    "X_train, y_train = X[0:int(len(X) * 0.8)], y[0:int(len(X) * 0.8)]\n",
    "X_test, y_test = X[int(len(X) * 0.8):], y[int(len(X) * 0.8):]\n",
    "\n",
    "# Predict the next number in the sequence\n",
    "model = GRU(input_size=window_size, hidden_size=5, output_size=1, learning_rate=0.000001,\n",
    "            output_activation=output_activation, leaky_relu_alpha=output_activation_alpha)\n",
    "model.train(X_train, y_train, epochs=400000, verbosity=1000, grad_clip_value=1)\n",
    "print('Test dataset validation')\n",
    "mae = np.sum(np.absolute((y_test.squeeze() - model.forward(X_test).squeeze()))) / len(y_test)\n",
    "print(f'MAE on test: {mae:.8f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
