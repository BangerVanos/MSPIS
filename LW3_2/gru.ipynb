{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate: float = 0.01,\n",
    "                 output_activation: Literal['linear', 'leaky_relu'] = 'linear',\n",
    "                 leaky_relu_alpha: float = 0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Инициализация весов для GRU\n",
    "        self.Wz = np.random.rand(hidden_size, input_size) * 0.01\n",
    "        self.Uz = np.random.rand(hidden_size, hidden_size) * 0.01\n",
    "        self.bz = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wh = np.random.rand(hidden_size, input_size) * 0.01\n",
    "        self.Uh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wr = np.random.rand(hidden_size, input_size) * 0.01\n",
    "        self.Ur = np.random.rand(hidden_size, hidden_size) * 0.01\n",
    "        self.br = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wo = np.random.rand(output_size, hidden_size) * 0.01\n",
    "        self.bo = np.zeros((output_size, 1))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Output neurons activation func + params\n",
    "        self.output_activation = output_activation\n",
    "        self.leaky_relu_alpha = leaky_relu_alpha\n",
    "\n",
    "    def activation(self, x):\n",
    "        # Гиперболический арксинус\n",
    "        return np.asinh(x)\n",
    "    \n",
    "    def leaky_relu(self, x, k: float = 0.01):\n",
    "        return np.where(x > 0, x, k * x)\n",
    "    \n",
    "    def linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.h = np.zeros((self.hidden_size, 1))\n",
    "        self.y = []\n",
    "        self.cache = []\n",
    "\n",
    "        for t in range(len(X)):\n",
    "            x_t = X[t].reshape(-1, 1)\n",
    "\n",
    "            # Грейдиентное обновление для GRU\n",
    "            z_t = self.sigmoid(self.Wz @ x_t + self.Uz @ self.h + self.bz)\n",
    "            r_t = self.sigmoid(self.Wr @ x_t + self.Ur @ self.h + self.br)\n",
    "            h_tilde = self.activation(self.Wh @ x_t + self.Uh @ (r_t * self.h) + self.bh)\n",
    "            h_next = (1 - z_t) * h_tilde + z_t * self.h\n",
    "            self.h = h_next\n",
    "\n",
    "            # Сохранение активаций для обратного распространения\n",
    "            self.cache.append((x_t, z_t, r_t, h_tilde, h_next))\n",
    "\n",
    "            # Выходное значение\n",
    "            y_t = self.Wo @ h_next + self.bo\n",
    "\n",
    "            if self.output_activation == 'linear':\n",
    "                y_t = self.linear(y_t)\n",
    "            elif self.output_activation == 'leaky_relu':\n",
    "                y_t = self.leaky_relu(y_t, self.leaky_relu_alpha)\n",
    "\n",
    "            self.y.append(y_t)\n",
    "\n",
    "        return np.array(self.y).squeeze(axis=-1)\n",
    "\n",
    "    def sigmoid(self, x):                \n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, X, Y, grad_clip_value=5.0):\n",
    "        dWz = np.zeros_like(self.Wz)\n",
    "        dUz = np.zeros_like(self.Uz)\n",
    "        dbz = np.zeros_like(self.bz)\n",
    "\n",
    "        dWh = np.zeros_like(self.Wh)\n",
    "        dUh = np.zeros_like(self.Uh)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "\n",
    "        dWr = np.zeros_like(self.Wr)\n",
    "        dUr = np.zeros_like(self.Ur)\n",
    "        dbr = np.zeros_like(self.br)\n",
    "\n",
    "        dWo = np.zeros_like(self.Wo)\n",
    "        dbo = np.zeros_like(self.bo)\n",
    "\n",
    "        dh_next = np.zeros_like(self.h)\n",
    "\n",
    "        # Обратное распространение ошибки по времени\n",
    "        for t in reversed(range(len(X))):\n",
    "            x_t, z_t, r_t, h_tilde, h_next = self.cache[t]\n",
    "            y_t = self.y[t]\n",
    "\n",
    "            # Ошибка на выходе\n",
    "            dy = y_t - Y[t]\n",
    "\n",
    "            # Градиенты для весов выхода\n",
    "            dWo += dy @ h_next.T\n",
    "            dbo += dy\n",
    "\n",
    "            # Градиенты для скрытого состояния\n",
    "            dh = self.Wo.T @ dy + dh_next\n",
    "            dh_tilde = dh * (1 - z_t)\n",
    "            dz = dh * (h_next - h_tilde)\n",
    "            dWh += dh_tilde * (1 - h_tilde ** 2) @ x_t.T\n",
    "            dUh += dh_tilde * (1 - h_tilde ** 2) @ (r_t * self.h).T\n",
    "            dbh += dh_tilde * (1 - h_tilde ** 2)\n",
    "\n",
    "            # Градиенты для обновлений\n",
    "            dUr += dz * r_t * self.h @ self.h.T\n",
    "            dWr += dz * r_t * self.h @ x_t.T\n",
    "            dbr += dz * r_t * self.h\n",
    "\n",
    "            # Градиенты для сброса и обновления\n",
    "            dUz += dz * (1 - z_t) @ self.h.T\n",
    "            dWz += dz * (1 - z_t) @ x_t.T\n",
    "            dbz += dz * (1 - z_t)\n",
    "\n",
    "            dh_next = (1 - z_t) * dh_tilde + z_t * dh\n",
    "        \n",
    "        # Ограничение градиентов, чтобы избежать взрыва\n",
    "        gradients = [dWz, dUz, dbz, dWh, dUh, dbh, dWr, dUr, dbr, dWo, dbo]\n",
    "        for grad in gradients:\n",
    "            np.clip(grad, -grad_clip_value, grad_clip_value, out=grad)\n",
    "\n",
    "        # Обновление весов\n",
    "        self.Wz -= self.learning_rate * dWz\n",
    "        self.Uz -= self.learning_rate * dUz\n",
    "        self.bz -= self.learning_rate * dbz\n",
    "\n",
    "        self.Wh -= self.learning_rate * dWh\n",
    "        self.Uh -= self.learning_rate * dUh\n",
    "        self.bh -= self.learning_rate * dbh\n",
    "\n",
    "        self.Wr -= self.learning_rate * dWr\n",
    "        self.Ur -= self.learning_rate * dUr\n",
    "        self.br -= self.learning_rate * dbr\n",
    "\n",
    "        self.Wo -= self.learning_rate * dWo\n",
    "        self.bo -= self.learning_rate * dbo\n",
    "\n",
    "    def train(self, X, Y, epochs=100, verbosity: int = 1, grad_clip_value: float = 5.0,\n",
    "              reset_hidden: bool = False):\n",
    "        for epoch in range(epochs):\n",
    "            # Reset hidden state each epoch if needed\n",
    "            if reset_hidden:\n",
    "                self.h = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "            self.forward(X)\n",
    "            self.backward(X, Y, grad_clip_value)\n",
    "            if epoch % verbosity == 0:\n",
    "                # MSE LOSS\n",
    "                # loss = np.mean((np.array(self.y) - Y) ** 2)\n",
    "\n",
    "                # MAE LOSS\n",
    "                loss = np.mean(np.absolute(np.array(self.y) - Y))\n",
    "                print(f'Epoch {epoch}/{epochs}, Loss: {loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data using sliding window\n",
    "def create_sliding_window_data(sequence, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence) - window_size):\n",
    "        X.append(sequence[i:i + window_size])\n",
    "        y.append(sequence[i + window_size])\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fibonacci sequence generator\n",
    "def fibonacci_generator(n):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        yield a\n",
    "        a, b = b, a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared num sequence generator\n",
    "def squared_generator(n, fst: float):\n",
    "    num = fst\n",
    "    for _ in range(n):\n",
    "        yield num\n",
    "        num = num**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared num sequence generator\n",
    "def half_generator(n, fst: float):\n",
    "    num = fst\n",
    "    for _ in range(n):\n",
    "        yield num\n",
    "        num /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/n sequence generator\n",
    "def one_by_n_generator(n):    \n",
    "    for i in range(n):\n",
    "        yield 1 / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, -1, 1, -1, 1,... sequence generator\n",
    "def plus_one_minus_one_generator(n):    \n",
    "    for i in range(n):        \n",
    "        yield 1 if i % 2 == 0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000, Loss: 0.040652\n",
      "Epoch 1000/10000, Loss: 0.028247\n",
      "Epoch 2000/10000, Loss: 0.028269\n",
      "Epoch 3000/10000, Loss: 0.028269\n",
      "Epoch 4000/10000, Loss: 0.028269\n",
      "Epoch 5000/10000, Loss: 0.028269\n",
      "Epoch 6000/10000, Loss: 0.028268\n",
      "Epoch 7000/10000, Loss: 0.028268\n",
      "Epoch 8000/10000, Loss: 0.028268\n",
      "Epoch 9000/10000, Loss: 0.028267\n"
     ]
    }
   ],
   "source": [
    "# Generate squared sequence\n",
    "n = 100\n",
    "sequence = list(one_by_n_generator(n))\n",
    "window_size = 3\n",
    "X, y = create_sliding_window_data(sequence, window_size)\n",
    "X_train, y_train = X[0:int(len(X) * 0.8)], y[0:int(len(X) * 0.8)]\n",
    "X_test, y_test = X[int(len(X) * 0.8):], y[int(len(X) * 0.8):]\n",
    "\n",
    "# Model params\n",
    "h = 5\n",
    "o = 1\n",
    "learning_rate = 0.0001\n",
    "output_activation = 'linear'\n",
    "output_activation_alpha = 0.01\n",
    "\n",
    "# Predict the next number in the sequence\n",
    "model = GRU(input_size=window_size, hidden_size=h, output_size=o, learning_rate=learning_rate,\n",
    "            output_activation=output_activation, leaky_relu_alpha=output_activation_alpha)\n",
    "model.train(X_train, y_train, epochs=10000, verbosity=1000, grad_clip_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset validation\n",
      "MAE on test: 0.02952825\n",
      "MAPE on test: 267.62557862%\n"
     ]
    }
   ],
   "source": [
    "# Assess model on test\n",
    "print('Test dataset validation')\n",
    "mae = np.mean(np.absolute((y_test.squeeze() - model.forward(X_test).squeeze())))\n",
    "print(f'MAE on test: {mae:.8f}')\n",
    "mape = np.mean(np.absolute((y_test.squeeze() - model.forward(X_test).squeeze())) / y_test.squeeze()) * 100\n",
    "print(f'MAPE on test: {mape:.8f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_n_elems(sequence, model, sequence_len, window_size, n):\n",
    "    predictions: list[float] = []\n",
    "    test_arr: list = sequence[sequence_len - window_size:sequence_len]\n",
    "    print(test_arr)\n",
    "    prediction = model.forward(np.array([test_arr]))\n",
    "    true_number = 1 / (sequence_len + 1)\n",
    "    pred_number = prediction.squeeze().item()\n",
    "    print(f'{np.abs(pred_number - true_number) / true_number * 100:.4f}%')\n",
    "    print(f'TRUE: {true_number}\\nPREDICTED: {pred_number}')\n",
    "    predictions.append(pred_number)\n",
    "    for i in range(1, n):\n",
    "        true_number = 1 / (sequence_len + i + 1)\n",
    "        \n",
    "        test_arr = sequence[sequence_len - window_size + i:sequence_len + i]        \n",
    "        test_arr.extend(predictions[-window_size:] if len(predictions) >= window_size else predictions)\n",
    "        print(test_arr)\n",
    "\n",
    "        prediction = model.forward(np.array([test_arr]))\n",
    "        pred_number = prediction.squeeze().item()\n",
    "        print(f'{np.abs(pred_number - true_number) / true_number * 100:.4f}%')\n",
    "        print(f'TRUE: {true_number}\\nPREDICTED: {pred_number}')\n",
    "        predictions.append(pred_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01020408163265306, 0.010101010101010102, 0.01]\n",
      "311.7303%\n",
      "TRUE: 0.009900990099009901\n",
      "PREDICTED: 0.04076538018430873\n",
      "[0.010101010101010102, 0.01, 0.04076538018430873]\n",
      "315.8706%\n",
      "TRUE: 0.00980392156862745\n",
      "PREDICTED: 0.04077162658434427\n",
      "[0.01, 0.04076538018430873, 0.04077162658434427]\n",
      "320.0176%\n",
      "TRUE: 0.009708737864077669\n",
      "PREDICTED: 0.04077840439143932\n",
      "[0.04076538018430873, 0.04077162658434427, 0.04077840439143932]\n",
      "324.1779%\n",
      "TRUE: 0.009615384615384616\n",
      "PREDICTED: 0.04078633201394373\n",
      "[0.04077162658434427, 0.04077840439143932, 0.04078633201394373]\n",
      "328.2565%\n",
      "TRUE: 0.009523809523809525\n",
      "PREDICTED: 0.04078633674255481\n"
     ]
    }
   ],
   "source": [
    "predict_next_n_elems(sequence, model, n, window_size, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311.7303%\n"
     ]
    }
   ],
   "source": [
    "prediction = model.forward(np.array([[1 / x for x in range(n - window_size + 1, n + 1)]]))\n",
    "true_number = 1 / (n + 1)\n",
    "pred_number = prediction.squeeze().item()\n",
    "print(f'{np.abs(pred_number - true_number) / true_number * 100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315.8706%\n"
     ]
    }
   ],
   "source": [
    "test_arr_2 = [1 / x for x in range(n - window_size + 2, n + 1)]\n",
    "test_arr_2.append(pred_number)\n",
    "true_number_2 = 1 / (n + 2)\n",
    "prediction_2 = model.forward(np.array([test_arr_2]))\n",
    "pred_number_2 = prediction_2.squeeze().item()\n",
    "print(f'{np.abs(pred_number_2 - true_number_2) / true_number_2 * 100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315.8706%\n"
     ]
    }
   ],
   "source": [
    "test_arr_2 = [1 / x for x in range(n - window_size + 2, n + 1)]\n",
    "test_arr_2.append(pred_number)\n",
    "true_number_2 = 1 / (n + 2)\n",
    "prediction_2 = model.forward(np.array([test_arr_2]))\n",
    "pred_number_2 = prediction_2.squeeze().item()\n",
    "print(f'{np.abs(pred_number_2 - true_number_2) / true_number_2 * 100:.4f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
