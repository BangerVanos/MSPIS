{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_window_batches(sequence, window_size, batch_size, output_size):\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Формирование данных скользящего окна\n",
    "    for i in range(len(sequence) - window_size - output_size + 1):\n",
    "        X.append(sequence[i:i + window_size])\n",
    "        y.append(sequence[i + window_size:i + window_size + output_size])\n",
    "    \n",
    "    # Преобразование в массивы numpy\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # Определение количества батчей\n",
    "    total_samples = len(X)\n",
    "    batch_amount = int(np.ceil(total_samples / batch_size))\n",
    "\n",
    "    # Дополнение последнего батча, если данных не хватает\n",
    "    if total_samples % batch_size != 0:\n",
    "        pad_size = batch_size - (total_samples % batch_size)\n",
    "        \n",
    "        X_pad = np.repeat(X[-1][np.newaxis, :], pad_size, axis=0)\n",
    "        y_pad = np.repeat(y[-1][np.newaxis, :], pad_size, axis=0)\n",
    "        \n",
    "        X = np.vstack((X, X_pad))\n",
    "        y = np.vstack((y, y_pad))\n",
    "\n",
    "    # Формирование батчей\n",
    "    X_batches = X.reshape(batch_amount, batch_size, window_size)\n",
    "    y_batches = y.reshape(batch_amount, batch_size, output_size)\n",
    "\n",
    "    return X_batches, y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fibonacci sequence generator\n",
    "def fibonacci_generator(n):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        yield a\n",
    "        a, b = b, a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared num sequence generator\n",
    "def squared_generator(n, fst: float):\n",
    "    num = fst\n",
    "    for _ in range(n):\n",
    "        yield num\n",
    "        num = num**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared num sequence generator\n",
    "def half_generator(n, fst: float):\n",
    "    num = fst\n",
    "    for _ in range(n):\n",
    "        yield num\n",
    "        num /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/n sequence generator\n",
    "def one_by_n_generator(n):    \n",
    "    for i in range(n):\n",
    "        yield 1 / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, -1, 1, -1, 1,... sequence generator\n",
    "def plus_one_minus_one_generator(n):    \n",
    "    for i in range(n):        \n",
    "        yield 1 if i % 2 == 0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arithmetic progression\n",
    "def arithmetic_progression(n, a0, d):\n",
    "    for i in range(n):\n",
    "        yield a0 + i * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,... sequence generator\n",
    "def one_zero_generator(n):\n",
    "    count = 1\n",
    "    generated = 0\n",
    "    while generated < n:\n",
    "        generated += 1\n",
    "        yield 1\n",
    "        for _ in range(count):\n",
    "            if generated >= n:\n",
    "                break\n",
    "            generated += 1\n",
    "            yield 0            \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arcsinh(x):\n",
    "    return np.arcsinh(x)\n",
    "\n",
    "def darcsinh(x):\n",
    "    # Производная arcsinh(x) = 1 / sqrt(x^2 + 1)\n",
    "    return 1.0 / np.sqrt(x**2 + 1.0)\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def mse_grad(y_pred, y_true):\n",
    "    return 2 * (y_pred - y_true) / y_pred.size\n",
    "\n",
    "def mape(y_true, y_pred) -> float:\n",
    "    return np.mean(np.absolute((y_true - y_pred) / y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        limit = np.sqrt(1.0 / hidden_size)\n",
    "        \n",
    "        # Инициализация параметров\n",
    "        self.W_z = np.random.uniform(-limit, limit, (input_size, hidden_size))\n",
    "        self.U_z = np.random.uniform(-limit, limit, (hidden_size, hidden_size))\n",
    "        self.b_z = np.zeros(hidden_size)\n",
    "        \n",
    "        self.W_r = np.random.uniform(-limit, limit, (input_size, hidden_size))\n",
    "        self.U_r = np.random.uniform(-limit, limit, (hidden_size, hidden_size))\n",
    "        self.b_r = np.zeros(hidden_size)\n",
    "        \n",
    "        self.W_h = np.random.uniform(-limit, limit, (input_size, hidden_size))\n",
    "        self.U_h = np.random.uniform(-limit, limit, (hidden_size, hidden_size))\n",
    "        self.b_h = np.zeros(hidden_size)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        \"\"\"\n",
    "        Возвращает h_t и словарь cache для обратного прохода.\n",
    "        \"\"\"\n",
    "        # Рассчёт гейтов\n",
    "        z_t_in = x_t @ self.W_z + h_prev @ self.U_z + self.b_z\n",
    "        z_t = self.sigmoid(z_t_in)\n",
    "        \n",
    "        r_t_in = x_t @ self.W_r + h_prev @ self.U_r + self.b_r\n",
    "        r_t = self.sigmoid(r_t_in)\n",
    "        \n",
    "        h_hat_t_in = x_t @ self.W_h + (r_t * h_prev) @ self.U_h + self.b_h\n",
    "        h_hat_t = arcsinh(h_hat_t_in)\n",
    "        \n",
    "        h_t = (1 - z_t)*h_prev + z_t*h_hat_t\n",
    "        \n",
    "        cache = {\n",
    "            'x_t': x_t, 'h_prev': h_prev,\n",
    "            'z_t': z_t, 'z_t_in': z_t_in,\n",
    "            'r_t': r_t, 'r_t_in': r_t_in,\n",
    "            'h_hat_t_in': h_hat_t_in, 'h_hat_t': h_hat_t,\n",
    "        }\n",
    "        return h_t, cache\n",
    "\n",
    "    def backward(self, dh_t, cache):\n",
    "        \"\"\"\n",
    "        dh_t: градиент по h_t (след. шага или по функции потерь)\n",
    "        Возвращает градиенты по параметрам и dh_prev, а также dx_t\n",
    "        \"\"\"\n",
    "        x_t = cache['x_t']\n",
    "        h_prev = cache['h_prev']\n",
    "        z_t = cache['z_t']\n",
    "        z_t_in = cache['z_t_in']\n",
    "        r_t = cache['r_t']\n",
    "        r_t_in = cache['r_t_in']\n",
    "        h_hat_t_in = cache['h_hat_t_in']\n",
    "        h_hat_t = cache['h_hat_t']\n",
    "\n",
    "        # dh_t по h_t:\n",
    "        # h_t = (1 - z_t)*h_prev + z_t*h_hat_t\n",
    "        # dh_prev_contrib = dh_t * (1 - z_t)\n",
    "        # dz_t = dh_t * (h_hat_t - h_prev)\n",
    "        # dh_hat_t = dh_t * z_t\n",
    "        \n",
    "        dh_hat_t = dh_t * z_t\n",
    "        dz_t = dh_t * (h_hat_t - h_prev)\n",
    "        dh_prev = dh_t * (1 - z_t)\n",
    "\n",
    "        # Производные через arcsinh:\n",
    "        # h_hat_t = arcsinh(h_hat_t_in)\n",
    "        # dh_hat_t_in = dh_hat_t * darcsinh(h_hat_t_in)\n",
    "        dh_hat_t_in = dh_hat_t * darcsinh(h_hat_t_in)\n",
    "        \n",
    "        # r_t = sigmoid(r_t_in)\n",
    "        # для зависимостей внутри h_hat_t_in:\n",
    "        # h_hat_t_in = x_t W_h + (r_t * h_prev) U_h + b_h\n",
    "        \n",
    "        # d(r_t * h_prev) = (r_t * h_prev) @ U_h\n",
    "        # Но нам нужна производная по r_t и h_prev.\n",
    "        # dh_hat_t_in wrt r_t: (h_prev @ U_h)\n",
    "        # dh_hat_t_in wrt h_prev (через r_t): (r_t @ U_h^T)\n",
    "        \n",
    "        # Сначала разложим градиенты по параметрам:\n",
    "        dW_h = x_t.T @ dh_hat_t_in\n",
    "        dU_h = (r_t * h_prev).T @ dh_hat_t_in\n",
    "        db_h = np.sum(dh_hat_t_in, axis=0)\n",
    "        \n",
    "        # Производим обратный проход по r_t:\n",
    "        # h_hat_t_in зависит от r_t: dh_hat_t_in/dr_t = (h_prev @ U_h)\n",
    "        # dr_t_in = d(r_t)/dr_t_in * ...\n",
    "        dr_t = (dh_hat_t_in @ self.U_h.T) * h_prev\n",
    "        # r_t = sigmoid(r_t_in) => dr_t_in = dr_t * r_t*(1-r_t)\n",
    "        dr_t_in = dr_t * r_t * (1 - r_t)\n",
    "        \n",
    "        # Производим обратный проход по h_prev из h_hat_t_in:\n",
    "        dh_prev += (dh_hat_t_in @ self.U_h.T) * r_t\n",
    "\n",
    "        # z_t = sigmoid(z_t_in)\n",
    "        # dz_t_in = dz_t * z_t*(1-z_t)\n",
    "        dz_t_in = dz_t * z_t * (1 - z_t)\n",
    "\n",
    "        # Теперь разберем x_t и h_prev воздействия для z_t и r_t:\n",
    "        dW_z = x_t.T @ dz_t_in\n",
    "        dU_z = h_prev.T @ dz_t_in\n",
    "        db_z = np.sum(dz_t_in, axis=0)\n",
    "        \n",
    "        dW_r = x_t.T @ dr_t_in\n",
    "        dU_r = h_prev.T @ dr_t_in\n",
    "        db_r = np.sum(dr_t_in, axis=0)\n",
    "\n",
    "        # Теперь учесть влияние z_t и r_t на h_prev, x_t:\n",
    "        # Часть dh_prev уже учтена:\n",
    "        # h_t зависит от h_prev через (1 - z_t)*h_prev => dh_prev += dh_t*(1-z_t)\n",
    "        # У нас уже это учтено выше.\n",
    "\n",
    "        # h_prev также влияет через z_t_in и r_t_in:\n",
    "        dh_prev += (dz_t_in @ self.U_z.T)\n",
    "        dh_prev += (dr_t_in @ self.U_r.T)\n",
    "\n",
    "        # h_prev влияет также через h_hat_t_in (уже учтено выше)\n",
    "        \n",
    "        # Для x_t:\n",
    "        dx_t = (dz_t_in @ self.W_z.T) + (dr_t_in @ self.W_r.T) + (dh_hat_t_in @ self.W_h.T)\n",
    "\n",
    "        return dx_t, dh_prev, (dW_z, dU_z, db_z, dW_r, dU_r, db_r, dW_h, dU_h, db_h)\n",
    "\n",
    "\n",
    "class GRUModel:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.cell = GRUCell(input_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Выходной слой: h_T -> y\n",
    "        limit = np.sqrt(1.0 / hidden_size)\n",
    "        self.W_out = np.random.uniform(-limit, limit, (hidden_size, output_size))\n",
    "        self.b_out = np.zeros(output_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (seq_length, batch_size, input_size)\n",
    "        Возвращает предсказание y_pred и кэш для обратного прохода.\n",
    "        y_pred будет рассчитываться по последнему скрытому состоянию.\n",
    "        \"\"\"\n",
    "        seq_length, batch_size, _ = X.shape\n",
    "        h = np.zeros((seq_length, batch_size, self.hidden_size))\n",
    "        h_prev = np.zeros((batch_size, self.hidden_size))\n",
    "        caches = []\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            h_t, cache_t = self.cell.forward(X[t], h_prev)\n",
    "            h[t] = h_t\n",
    "            h_prev = h_t\n",
    "            caches.append(cache_t)\n",
    "        \n",
    "        # Предсказание по последнему состоянию:\n",
    "        y_pred = h[-1] @ self.W_out + self.b_out\n",
    "        \n",
    "        return y_pred, h, caches\n",
    "    \n",
    "    def backward(self, dy_pred, h, caches, X):\n",
    "        \"\"\"\n",
    "        Выполняем обратный проход по всей последовательности (BPTT).\n",
    "        dy_pred: градиент по выходу (на последнем шаге)\n",
    "        \"\"\"\n",
    "        seq_length, batch_size, _ = X.shape\n",
    "        \n",
    "        dW_z = np.zeros_like(self.cell.W_z)\n",
    "        dU_z = np.zeros_like(self.cell.U_z)\n",
    "        db_z = np.zeros_like(self.cell.b_z)\n",
    "        dW_r = np.zeros_like(self.cell.W_r)\n",
    "        dU_r = np.zeros_like(self.cell.U_r)\n",
    "        db_r = np.zeros_like(self.cell.b_r)\n",
    "        dW_h = np.zeros_like(self.cell.W_h)\n",
    "        dU_h = np.zeros_like(self.cell.U_h)\n",
    "        db_h = np.zeros_like(self.cell.b_h)\n",
    "        \n",
    "        dW_out = np.zeros_like(self.W_out)\n",
    "        db_out = np.zeros_like(self.b_out)\n",
    "        \n",
    "        dh_prev = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        # Градиенты по выходу:\n",
    "        # y_pred = h[-1] @ W_out + b_out\n",
    "        # dy_pred уже дан\n",
    "        dW_out = h[-1].T @ dy_pred\n",
    "        db_out = np.sum(dy_pred, axis=0)\n",
    "        \n",
    "        # dh_last:\n",
    "        dh = dy_pred @ self.W_out.T\n",
    "        dh += dh_prev\n",
    "        \n",
    "        # Обратный проход по слоям GRU\n",
    "        for t in reversed(range(seq_length)):\n",
    "            dx_t, dh_prev, grads_cell = self.cell.backward(dh, caches[t])\n",
    "            gW_z, gU_z, gb_z, gW_r, gU_r, gb_r, gW_h, gU_h, gb_h = grads_cell\n",
    "            \n",
    "            # Суммируем градиенты\n",
    "            dW_z += gW_z\n",
    "            dU_z += gU_z\n",
    "            db_z += gb_z\n",
    "            dW_r += gW_r\n",
    "            dU_r += gU_r\n",
    "            db_r += gb_r\n",
    "            dW_h += gW_h\n",
    "            dU_h += gU_h\n",
    "            db_h += gb_h\n",
    "            \n",
    "            # Для следующих шагов:\n",
    "            # dh_prev уже обновлен в cell.backward\n",
    "            # dx_t мы не используем для обновления, так как вход не обучаем.\n",
    "            \n",
    "            # Если не последний слой, dh придёт с предыдущей итерации\n",
    "            if t > 0:\n",
    "                dh = dh_prev\n",
    "            else:\n",
    "                # Первый шаг последовательности, dh_prev здесь больше не нужен\n",
    "                pass\n",
    "        \n",
    "        grads = {\n",
    "            'W_z': dW_z, 'U_z': dU_z, 'b_z': db_z,\n",
    "            'W_r': dW_r, 'U_r': dU_r, 'b_r': db_r,\n",
    "            'W_h': dW_h, 'U_h': dU_h, 'b_h': db_h,\n",
    "            'W_out': dW_out, 'b_out': db_out\n",
    "        }\n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads, lr=0.01):\n",
    "        # Обновление параметров с помощью простого SGD\n",
    "        self.cell.W_z -= lr * grads['W_z']\n",
    "        self.cell.U_z -= lr * grads['U_z']\n",
    "        self.cell.b_z -= lr * grads['b_z']\n",
    "        \n",
    "        self.cell.W_r -= lr * grads['W_r']\n",
    "        self.cell.U_r -= lr * grads['U_r']\n",
    "        self.cell.b_r -= lr * grads['b_r']\n",
    "        \n",
    "        self.cell.W_h -= lr * grads['W_h']\n",
    "        self.cell.U_h -= lr * grads['U_h']\n",
    "        self.cell.b_h -= lr * grads['b_h']\n",
    "        \n",
    "        self.W_out -= lr * grads['W_out']\n",
    "        self.b_out -= lr * grads['b_out']\n",
    "\n",
    "    def train(self, x, y, lr: float = 0.01, max_epochs: int = 10000,\n",
    "              learn_by_loss: bool = False, max_loss: float = 0.01,\n",
    "              verbosity: int = 1000):        \n",
    "        for epoch in range(max_epochs):\n",
    "            # Прямой проход\n",
    "            y_pred, h, caches = self.forward(x)\n",
    "            loss = mse_loss(y_pred, y)\n",
    "            \n",
    "            # Обратный проход\n",
    "            dy_pred = mse_grad(y_pred, y)\n",
    "            grads = self.backward(dy_pred, h, caches, x)\n",
    "            \n",
    "            # Обновление параметров\n",
    "            self.update_parameters(grads, lr)\n",
    "            \n",
    "            epoch_mape = mape(y, y_pred)\n",
    "            if (epoch+1) % verbosity == 0:\n",
    "                print(f\"Epoch {epoch+1}/{max_epochs}, Loss: {loss:.6f}\\nMAPE: {epoch_mape:.6f}\")\n",
    "\n",
    "            if learn_by_loss and epoch_mape <= max_loss:\n",
    "                print('TRAINING FINISHED')\n",
    "                print(f\"Epoch {epoch+1}/{max_epochs}, Loss: {loss:.6f}\\nMAPE: {epoch_mape:.6f}\")\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/10000, Loss: 0.110879\n",
      "MAPE: 0.022262\n",
      "Epoch 2000/10000, Loss: 0.109610\n",
      "MAPE: 0.022134\n",
      "Epoch 3000/10000, Loss: 0.108011\n",
      "MAPE: 0.021971\n",
      "Epoch 4000/10000, Loss: 0.105871\n",
      "MAPE: 0.021752\n",
      "Epoch 5000/10000, Loss: 0.102819\n",
      "MAPE: 0.021435\n",
      "Epoch 6000/10000, Loss: 0.098326\n",
      "MAPE: 0.020960\n",
      "Epoch 7000/10000, Loss: 0.092464\n",
      "MAPE: 0.020323\n",
      "Epoch 8000/10000, Loss: 0.086762\n",
      "MAPE: 0.019685\n",
      "Epoch 9000/10000, Loss: 0.081307\n",
      "MAPE: 0.019057\n",
      "Epoch 10000/10000, Loss: 0.075532\n",
      "MAPE: 0.018367\n"
     ]
    }
   ],
   "source": [
    "# Пример обучения:\n",
    "# Задача: предсказать следующий элемент последовательности.\n",
    "# Допустим, у нас есть простая последовательность вида: X_t = t*0.1\n",
    "# Будем пробовать прогнозировать следующий шаг.\n",
    "\n",
    "seq_length = 20\n",
    "window_size = 5\n",
    "batch_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "sequence = list(arithmetic_progression(seq_length, 1, 1))\n",
    "X, y = create_sliding_window_batches(sequence, window_size, batch_size, output_size)\n",
    "X_train, y_train = X[0:int(len(X) * 0.8)], y[0:int(len(X) * 0.8)][-1]\n",
    "X_test, y_test = X[int(len(X) * 0.8):], y[int(len(X) * 0.8):][-1]\n",
    "\n",
    "verbosity = 1000\n",
    "\n",
    "max_mape = 1e-3\n",
    "learn_until_meet_mape = True\n",
    "\n",
    "model = GRUModel(window_size, hidden_size, output_size)\n",
    "\n",
    "# Параметры обучения\n",
    "lr = 1e-3\n",
    "epochs = 10000\n",
    "\n",
    "model.train(X_train, y_train, lr, epochs,\n",
    "            True, max_mape, verbosity)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказание после обучения: [[14.27692316 15.27509486]\n",
      " [14.7255928  15.72715828]]\n",
      "Истинное значение: [[14 15]\n",
      " [15 16]]\n",
      "MAPE: 1.8367%\n",
      "TEST\n",
      "Предсказания на тестовой выборке: [[15.9500036  16.94873542]\n",
      " [16.20427453 17.21613161]]\n",
      "Истинное значение на тестовой выборке: [[18 19]\n",
      " [19 20]]\n",
      "MAPE: 12.7047%\n"
     ]
    }
   ],
   "source": [
    "# Проверим результат после обучения\n",
    "y_pred, _, _ = model.forward(X_train)\n",
    "print(\"Предсказание после обучения:\", y_pred)\n",
    "print(\"Истинное значение:\", y_train)\n",
    "print(f\"MAPE: {mape(y_train, y_pred) * 100:.4f}%\")\n",
    "\n",
    "print('TEST')\n",
    "y_pred, _, _ = model.forward(X_test)\n",
    "print(\"Предсказания на тестовой выборке:\", y_pred)\n",
    "print(\"Истинное значение на тестовой выборке:\", y_test)\n",
    "print(f\"MAPE: {mape(y_test, y_pred) * 100:.4f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
