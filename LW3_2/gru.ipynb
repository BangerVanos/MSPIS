{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лабораторная работа 3 по дисциплине МРЗвИС\n",
    "# Выполнена студентом группы 121702\n",
    "# БГУИР Заломов Роман Андреевич\n",
    "#\n",
    "# Вариант 15: Реализовать модель рекуррентной сети с цепью нейросетевых моделей управляемых рекуррентных блоков \n",
    "# с логарифмической функцией активации (гиперболический арксинус) выходного сигнала на скрытом слое\n",
    "#\n",
    "# 24.12.24\n",
    "# Данный файл содержит реализацию сети GRU\n",
    "# с функцией активации arcsinh на скрытом слое\n",
    "# Исходный код, отвечающий за реализацию модели сети, был взят с: https://pastebin.com/UniAESGy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Literal\n",
    "# from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from gru_with_adam import GRUAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_window_batches(sequence, window_size, batch_size, output_size):\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Формирование данных скользящего окна\n",
    "    for i in range(len(sequence) - window_size - output_size + 1):\n",
    "        X.append(sequence[i:i + window_size])\n",
    "        y.append(sequence[i + window_size:i + window_size + output_size])\n",
    "    \n",
    "    # Преобразование в массивы numpy\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # Определение количества батчей\n",
    "    total_samples = len(X)\n",
    "    batch_amount = int(np.ceil(total_samples / batch_size))\n",
    "\n",
    "    # Дополнение последнего батча, если данных не хватает\n",
    "    if total_samples % batch_size != 0:\n",
    "        pad_size = batch_size - (total_samples % batch_size)\n",
    "        \n",
    "        X_pad = np.repeat(X[-1][np.newaxis, :], pad_size, axis=0)\n",
    "        y_pad = np.repeat(y[-1][np.newaxis, :], pad_size, axis=0)\n",
    "        \n",
    "        X = np.vstack((X, X_pad))\n",
    "        y = np.vstack((y, y_pad))\n",
    "\n",
    "    # Формирование батчей\n",
    "    X_batches = X.reshape(batch_amount, batch_size, window_size)\n",
    "    y_batches = y.reshape(batch_amount, batch_size, output_size)\n",
    "\n",
    "    return X_batches, y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fibonacci sequence generator\n",
    "def fibonacci_generator(n):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        yield a\n",
    "        a, b = b, a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared num sequence generator\n",
    "def squared_generator(n):\n",
    "    for i in range(1, n + 1):\n",
    "        yield i**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared num sequence generator\n",
    "def half_generator(n, fst: float):\n",
    "    num = fst\n",
    "    for _ in range(n):\n",
    "        yield num\n",
    "        num /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/n sequence generator\n",
    "def one_by_n_generator(n):    \n",
    "    for i in range(n):\n",
    "        yield 1 / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, -1, 1, -1, 1,... sequence generator\n",
    "def plus_one_minus_one_generator(n):    \n",
    "    for i in range(n):        \n",
    "        yield 1 if i % 2 == 0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arithmetic progression\n",
    "def arithmetic_progression(n, a0, d):\n",
    "    for i in range(n):\n",
    "        yield a0 + i * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, 0.5, 1, 0.5, 0.5, 1, 0.5, 0.5, 0.5, 1,... sequence generator\n",
    "def one_half_generator(n):\n",
    "    count = 1\n",
    "    generated = 0\n",
    "    while generated < n:\n",
    "        generated += 1\n",
    "        yield 1\n",
    "        for _ in range(count):\n",
    "            if generated >= n:\n",
    "                break\n",
    "            generated += 1\n",
    "            yield 0.5            \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arcsinh(x):\n",
    "    return np.arcsinh(x)\n",
    "\n",
    "def darcsinh(x):\n",
    "    # Производная arcsinh(x) = 1 / sqrt(x^2 + 1)\n",
    "    return 1.0 / np.sqrt(x**2 + 1.0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(x):\n",
    "    # Производная arcsinh(x) = 1 / sqrt(x^2 + 1)\n",
    "    return 1.0 - tanh(x) ** 2\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return 1/2 * (y_true - y_pred) ** 2    \n",
    "\n",
    "def mse_grad(y_pred, y_true):\n",
    "    return y_pred - y_true    \n",
    "\n",
    "def mape(y_true, y_pred, ignore_zero: bool = True) -> float:\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)        \n",
    "    if ignore_zero:\n",
    "        # Avoiding devision by zero       \n",
    "        mask = y_true != 0        \n",
    "        y_true = y_true[mask]\n",
    "        y_pred = y_pred[mask]      \n",
    "    return np.mean(np.absolute((y_true - y_pred) / y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def mse_loss(y_pred, y_true):\n",
    "    # return 1/2 * np.sum((y_true - y_pred) ** 2)\n",
    "    return  np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def mse_grad(y_pred, y_true):\n",
    "    # return np.sum(y_pred - y_true)\n",
    "    return 2 * (y_pred - y_true) / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        limit = np.sqrt(1.0 / hidden_size)\n",
    "        \n",
    "        # Инициализация параметров\n",
    "        self.W_z = np.random.uniform(-limit, limit, (input_size, hidden_size))\n",
    "        self.U_z = np.random.uniform(-limit, limit, (hidden_size, hidden_size))\n",
    "        self.b_z = np.zeros(hidden_size)\n",
    "        \n",
    "        self.W_r = np.random.uniform(-limit, limit, (input_size, hidden_size))\n",
    "        self.U_r = np.random.uniform(-limit, limit, (hidden_size, hidden_size))\n",
    "        self.b_r = np.zeros(hidden_size)\n",
    "        \n",
    "        self.W_h = np.random.uniform(-limit, limit, (input_size, hidden_size))\n",
    "        self.U_h = np.random.uniform(-limit, limit, (hidden_size, hidden_size))\n",
    "        self.b_h = np.zeros(hidden_size)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        \"\"\"\n",
    "        Возвращает h_t и словарь cache для обратного прохода.\n",
    "        \"\"\"\n",
    "        # Рассчёт гейтов\n",
    "        z_t_in = x_t @ self.W_z + h_prev @ self.U_z + self.b_z\n",
    "        z_t = self.sigmoid(z_t_in)\n",
    "        \n",
    "        r_t_in = x_t @ self.W_r + h_prev @ self.U_r + self.b_r\n",
    "        r_t = self.sigmoid(r_t_in)\n",
    "        \n",
    "        h_hat_t_in = x_t @ self.W_h + (r_t * h_prev) @ self.U_h + self.b_h\n",
    "        h_hat_t = arcsinh(h_hat_t_in)\n",
    "        \n",
    "        h_t = (1 - z_t)*h_prev + z_t*h_hat_t\n",
    "        \n",
    "        cache = {\n",
    "            'x_t': x_t, 'h_prev': h_prev,\n",
    "            'z_t': z_t, 'z_t_in': z_t_in,\n",
    "            'r_t': r_t, 'r_t_in': r_t_in,\n",
    "            'h_hat_t_in': h_hat_t_in, 'h_hat_t': h_hat_t,\n",
    "        }\n",
    "        return h_t, cache\n",
    "\n",
    "    def backward(self, dh_t, cache):\n",
    "        \"\"\"\n",
    "        dh_t: градиент по h_t (след. шага или по функции потерь)\n",
    "        Возвращает градиенты по параметрам и dh_prev, а также dx_t\n",
    "        \"\"\"\n",
    "        x_t = cache['x_t']\n",
    "        h_prev = cache['h_prev']\n",
    "        z_t = cache['z_t']\n",
    "        z_t_in = cache['z_t_in']\n",
    "        r_t = cache['r_t']\n",
    "        r_t_in = cache['r_t_in']\n",
    "        h_hat_t_in = cache['h_hat_t_in']\n",
    "        h_hat_t = cache['h_hat_t']\n",
    "\n",
    "        # dh_t по h_t:\n",
    "        # h_t = (1 - z_t)*h_prev + z_t*h_hat_t\n",
    "        # dh_prev_contrib = dh_t * (1 - z_t)\n",
    "        # dz_t = dh_t * (h_hat_t - h_prev)\n",
    "        # dh_hat_t = dh_t * z_t\n",
    "        \n",
    "        dh_hat_t = dh_t * z_t\n",
    "        dz_t = dh_t * (h_hat_t - h_prev)\n",
    "        dh_prev = dh_t * (1 - z_t)\n",
    "\n",
    "        # Производные через arcsinh:\n",
    "        # h_hat_t = arcsinh(h_hat_t_in)\n",
    "        # dh_hat_t_in = dh_hat_t * darcsinh(h_hat_t_in)\n",
    "        dh_hat_t_in = dh_hat_t * darcsinh(h_hat_t_in)\n",
    "        \n",
    "        # r_t = sigmoid(r_t_in)\n",
    "        # для зависимостей внутри h_hat_t_in:\n",
    "        # h_hat_t_in = x_t W_h + (r_t * h_prev) U_h + b_h\n",
    "        \n",
    "        # d(r_t * h_prev) = (r_t * h_prev) @ U_h\n",
    "        # Но нам нужна производная по r_t и h_prev.\n",
    "        # dh_hat_t_in wrt r_t: (h_prev @ U_h)\n",
    "        # dh_hat_t_in wrt h_prev (через r_t): (r_t @ U_h^T)\n",
    "        \n",
    "        # Сначала разложим градиенты по параметрам:\n",
    "        dW_h = x_t.T @ dh_hat_t_in\n",
    "        dU_h = (r_t * h_prev).T @ dh_hat_t_in\n",
    "        db_h = np.sum(dh_hat_t_in, axis=0)\n",
    "        \n",
    "        # Производим обратный проход по r_t:\n",
    "        # h_hat_t_in зависит от r_t: dh_hat_t_in/dr_t = (h_prev @ U_h)\n",
    "        # dr_t_in = d(r_t)/dr_t_in * ...\n",
    "        dr_t = (dh_hat_t_in @ self.U_h.T) * h_prev\n",
    "        # r_t = sigmoid(r_t_in) => dr_t_in = dr_t * r_t*(1-r_t)\n",
    "        dr_t_in = dr_t * r_t * (1 - r_t)\n",
    "        \n",
    "        # Производим обратный проход по h_prev из h_hat_t_in:\n",
    "        dh_prev += (dh_hat_t_in @ self.U_h.T) * r_t\n",
    "\n",
    "        # z_t = sigmoid(z_t_in)\n",
    "        # dz_t_in = dz_t * z_t*(1-z_t)\n",
    "        dz_t_in = dz_t * z_t * (1 - z_t)\n",
    "\n",
    "        # Теперь разберем x_t и h_prev воздействия для z_t и r_t:\n",
    "        dW_z = x_t.T @ dz_t_in\n",
    "        dU_z = h_prev.T @ dz_t_in\n",
    "        db_z = np.sum(dz_t_in, axis=0)\n",
    "        \n",
    "        dW_r = x_t.T @ dr_t_in\n",
    "        dU_r = h_prev.T @ dr_t_in\n",
    "        db_r = np.sum(dr_t_in, axis=0)\n",
    "\n",
    "        # Теперь учесть влияние z_t и r_t на h_prev, x_t:\n",
    "        # Часть dh_prev уже учтена:\n",
    "        # h_t зависит от h_prev через (1 - z_t)*h_prev => dh_prev += dh_t*(1-z_t)\n",
    "        # У нас уже это учтено выше.\n",
    "\n",
    "        # h_prev также влияет через z_t_in и r_t_in:\n",
    "        dh_prev += (dz_t_in @ self.U_z.T)\n",
    "        dh_prev += (dr_t_in @ self.U_r.T)\n",
    "\n",
    "        # h_prev влияет также через h_hat_t_in (уже учтено выше)\n",
    "        \n",
    "        # Для x_t:\n",
    "        dx_t = (dz_t_in @ self.W_z.T) + (dr_t_in @ self.W_r.T) + (dh_hat_t_in @ self.W_h.T)\n",
    "\n",
    "        return dx_t, dh_prev, (dW_z, dU_z, db_z, dW_r, dU_r, db_r, dW_h, dU_h, db_h)\n",
    "\n",
    "\n",
    "class GRUModel:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.cell = GRUCell(input_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Выходной слой: h_T -> y\n",
    "        limit = np.sqrt(1.0 / hidden_size)\n",
    "        self.W_out = np.random.uniform(-limit, limit, (hidden_size, output_size))\n",
    "        self.b_out = np.zeros(output_size)        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (seq_length, batch_size, input_size)\n",
    "        Возвращает предсказание y_pred для каждого временного шага и кэш для обратного прохода.\n",
    "        y_pred теперь рассчитывается для всех временных шагов.\n",
    "        \"\"\"\n",
    "        seq_length, batch_size, _ = X.shape\n",
    "        h = np.zeros((seq_length, batch_size, self.hidden_size))  # Все скрытые состояния\n",
    "        y_pred = np.zeros((seq_length, batch_size, self.output_size))  # Предсказания для каждого шага\n",
    "        h_prev = np.zeros((batch_size, self.hidden_size))  # Инициализация предыдущего состояния\n",
    "        caches = []\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            h_t, cache_t = self.cell.forward(X[t], h_prev)  # Вычисляем скрытое состояние\n",
    "            h[t] = h_t\n",
    "            h_prev = h_t\n",
    "            caches.append(cache_t)\n",
    "            \n",
    "            # Вычисляем предсказание на текущем временном шаге:\n",
    "            y_pred[t] = h_t @ self.W_out + self.b_out\n",
    "\n",
    "        return y_pred, h, caches\n",
    "\n",
    "    \n",
    "    def backward(self, dy_pred, h, caches, X):\n",
    "        \"\"\"\n",
    "        Выполняем обратный проход по всей последовательности (BPTT).\n",
    "        dy_pred: (seq_length, batch_size, output_size) - градиент по выходу на каждом шаге.\n",
    "        \"\"\"\n",
    "        seq_length, batch_size, _ = X.shape\n",
    "\n",
    "        # Инициализация градиентов\n",
    "        dW_z = np.zeros_like(self.cell.W_z)\n",
    "        dU_z = np.zeros_like(self.cell.U_z)\n",
    "        db_z = np.zeros_like(self.cell.b_z)\n",
    "        dW_r = np.zeros_like(self.cell.W_r)\n",
    "        dU_r = np.zeros_like(self.cell.U_r)\n",
    "        db_r = np.zeros_like(self.cell.b_r)\n",
    "        dW_h = np.zeros_like(self.cell.W_h)\n",
    "        dU_h = np.zeros_like(self.cell.U_h)\n",
    "        db_h = np.zeros_like(self.cell.b_h)\n",
    "\n",
    "        dW_out = np.zeros_like(self.W_out)\n",
    "        db_out = np.zeros_like(self.b_out)\n",
    "\n",
    "        dh_next = np.zeros((batch_size, self.hidden_size))  # Градиент скрытого состояния для следующего шага\n",
    "\n",
    "        # Градиенты по выходу (y_pred = h @ W_out + b_out)\n",
    "        for t in reversed(range(seq_length)):\n",
    "            dW_out += h[t].T @ dy_pred[t]\n",
    "            db_out += np.sum(dy_pred[t], axis=0)\n",
    "\n",
    "            # Градиент скрытого состояния через выход\n",
    "            dh = dy_pred[t] @ self.W_out.T + dh_next\n",
    "\n",
    "            # Обратный проход через GRU для текущего временного шага\n",
    "            dx_t, dh_next, grads_cell = self.cell.backward(dh, caches[t])\n",
    "            gW_z, gU_z, gb_z, gW_r, gU_r, gb_r, gW_h, gU_h, gb_h = grads_cell\n",
    "\n",
    "            # Суммируем градиенты\n",
    "            dW_z += gW_z\n",
    "            dU_z += gU_z\n",
    "            db_z += gb_z\n",
    "            dW_r += gW_r\n",
    "            dU_r += gU_r\n",
    "            db_r += gb_r\n",
    "            dW_h += gW_h\n",
    "            dU_h += gU_h\n",
    "            db_h += gb_h\n",
    "\n",
    "        # Сборка всех градиентов в словарь\n",
    "        grads = {\n",
    "            'W_z': dW_z, 'U_z': dU_z, 'b_z': db_z,\n",
    "            'W_r': dW_r, 'U_r': dU_r, 'b_r': db_r,\n",
    "            'W_h': dW_h, 'U_h': dU_h, 'b_h': db_h,\n",
    "            'W_out': dW_out, 'b_out': db_out\n",
    "        }\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def update_parameters(self, grads, lr=0.01):\n",
    "        # Обновление параметров с помощью простого SGD\n",
    "        self.cell.W_z -= lr * grads['W_z']\n",
    "        self.cell.U_z -= lr * grads['U_z']\n",
    "        self.cell.b_z -= lr * grads['b_z']\n",
    "        \n",
    "        self.cell.W_r -= lr * grads['W_r']\n",
    "        self.cell.U_r -= lr * grads['U_r']\n",
    "        self.cell.b_r -= lr * grads['b_r']\n",
    "        \n",
    "        self.cell.W_h -= lr * grads['W_h']\n",
    "        self.cell.U_h -= lr * grads['U_h']\n",
    "        self.cell.b_h -= lr * grads['b_h']\n",
    "        \n",
    "        self.W_out -= lr * grads['W_out']\n",
    "        self.b_out -= lr * grads['b_out']\n",
    "    \n",
    "    def train(self, x, y, lr: float = 0.01, max_epochs: int = 10000,\n",
    "          learn_by_loss: bool = False, max_loss: float = 0.01,\n",
    "          verbosity: int = 1000):\n",
    "        \"\"\"\n",
    "        Обучение модели по заданным данным.\n",
    "        x: (seq_length, batch_size, input_size) - входные данные\n",
    "        y: (seq_length, batch_size, output_size) - истинные значения\n",
    "        \"\"\"\n",
    "        training_loss, training_mape = 0, 0\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            # Прямой проход\n",
    "            y_pred, h, caches = self.forward(x)\n",
    "            \n",
    "            # Вычисление функции потерь по всем временным шагам\n",
    "            loss = mse_loss(y_pred, y)  # y_pred и y имеют размерности (seq_length, batch_size, output_size)\n",
    "            \n",
    "            # Обратный проход\n",
    "            dy_pred = mse_grad(y_pred, y)  # Градиент потерь            \n",
    "            grads = self.backward(dy_pred, h, caches, x)\n",
    "            \n",
    "            # Обновление параметров\n",
    "            self.update_parameters(grads, lr)\n",
    "            \n",
    "            # Расчет метрики MAPE (по всем временным шагам)            \n",
    "            epoch_mape = mape(y, y_pred)\n",
    "            \n",
    "            if (epoch + 1) % verbosity == 0:\n",
    "                print(f\"Epoch {epoch+1}/{max_epochs}, Loss: {loss:.6f}\\nMAPE: {epoch_mape * 100:.6f}%\")\n",
    "            \n",
    "            # Условие остановки\n",
    "            if learn_by_loss and loss <= max_loss:\n",
    "                break\n",
    "        \n",
    "        y_pred, _, _ = self.forward(x)        \n",
    "        training_loss = mse_loss(y_pred, y)\n",
    "        training_mape = mape(y, y_pred)\n",
    "\n",
    "        # Итоговые результаты обучения\n",
    "        print('TRAINING FINISHED')\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs}, Loss: {training_loss:.6f}\\nMAPE: {training_mape * 100:.6f}%\")\n",
    "\n",
    "        # Возврат результатов обучения\n",
    "        return training_loss, training_mape                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_sequence(sequence):\n",
    "    mean = np.mean(sequence)\n",
    "    deviation = np.std(sequence)    \n",
    "    scaled = (np.array(sequence) - mean) / deviation    \n",
    "    return scaled, mean, deviation\n",
    "\n",
    "def descale_sequence(scaled, mean, deviation):\n",
    "    descaled = scaled * deviation + mean\n",
    "    return descaled\n",
    "\n",
    "def predict_next_n_elements(sequence, window_size, batch_size, hidden_size, n,\n",
    "                            max_epochs, verbosity, lr, use_adam: bool = False):\n",
    "    scaled, mean, deviation = scale_sequence(sequence)        \n",
    "    # scaled = sequence  \n",
    "    X, y = create_sliding_window_batches(scaled, window_size, batch_size, n)\n",
    "    if not use_adam:\n",
    "        model = GRUModel(window_size, hidden_size, n)\n",
    "    else:\n",
    "        model = GRUAdam(window_size, hidden_size, n)\n",
    "    model.train(X, y, lr, max_epochs, verbosity=verbosity)\n",
    "    train_pred, _, _ = model.forward(X)\n",
    "    training_mape = mape(y, train_pred)           \n",
    "    x_last = np.vstack(tuple(scaled[-window_size:] for _ in range(batch_size)))\n",
    "    x_last = np.expand_dims(x_last, axis=0)    \n",
    "    pred, _, _ = model.forward(x_last)\n",
    "    descaled_pred = descale_sequence(pred[-1][-1], mean, deviation)\n",
    "    # descaled_pred = pred[-1][-1]\n",
    "    return training_mape, model, descaled_pred, mean, deviation\n",
    "\n",
    "# def predict_next_n_elements(sequence, window_size, batch_size, hidden_size, n,\n",
    "#                             max_epochs, verbosity, lr):\n",
    "#     scaler = RobustScaler()    \n",
    "#     scaled = scaler.fit_transform(np.array(sequence).reshape(-1, 1))\n",
    "#     scaled = np.squeeze(scaled)           \n",
    "#     X, y = create_sliding_window_batches(scaled, window_size, batch_size, n)\n",
    "#     model = GRUModel(window_size, hidden_size, n)\n",
    "#     model.train(X, y, lr, max_epochs, verbosity=verbosity)\n",
    "#     train_pred, _, _ = model.forward(X)\n",
    "#     training_mape = mape(y, train_pred)           \n",
    "#     x_last = np.vstack(tuple(scaled[-window_size:] for _ in range(batch_size)))\n",
    "#     x_last = np.expand_dims(x_last, axis=0)    \n",
    "#     pred, _, _ = model.forward(x_last)\n",
    "#     descaled_pred = np.squeeze(scaler.inverse_transform(pred[-1][-1].reshape(-1, 1)))    \n",
    "#     return training_mape, model, descaled_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2500/10000, Loss: 0.046240\n",
      "MAPE: 21.104917%\n",
      "Epoch 5000/10000, Loss: 0.006345\n",
      "MAPE: 3.638497%\n",
      "Epoch 7500/10000, Loss: 0.005530\n",
      "MAPE: 3.100775%\n",
      "Epoch 10000/10000, Loss: 0.004843\n",
      "MAPE: 2.867873%\n",
      "TRAINING FINISHED\n",
      "Epoch 10000/10000, Loss: 0.004842\n",
      "MAPE: 2.867785%\n",
      "Model performance on training (MAPE): 2.8678%\n",
      "Elems of test sequence: [1, -1, 1]\n",
      "Next 3 elements of sequence (predicted): [ 1.27806157 -1.38572364  1.39060396]\n",
      "MAPE of prediction: 35.1463%\n"
     ]
    }
   ],
   "source": [
    "seq_length = 40\n",
    "window_size = 8\n",
    "batch_size = 1\n",
    "hidden_size = 10\n",
    "output_size = 3\n",
    "\n",
    "learning_rate = 1e-3\n",
    "max_epochs = 10_000\n",
    "verbosity = max_epochs // 4\n",
    "use_adam = False\n",
    "\n",
    "sequence = list(plus_one_minus_one_generator(seq_length + output_size))\n",
    "train_sequence = sequence[:seq_length]\n",
    "test_sequence = sequence[-output_size:]\n",
    "training_mape, model, next_n_pred, mean, deviation = predict_next_n_elements(\n",
    "    train_sequence, window_size, batch_size,\n",
    "    hidden_size, output_size, max_epochs,\n",
    "    verbosity, learning_rate, use_adam=use_adam\n",
    ")\n",
    "print(f'Model performance on training (MAPE): {training_mape * 100:.4f}%')\n",
    "print(f'Elems of test sequence: {test_sequence}')\n",
    "print(f'Next {output_size} elements of sequence (predicted): {next_n_pred}')\n",
    "print(f'MAPE of prediction: {mape(test_sequence, next_n_pred) * 100:.4f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
